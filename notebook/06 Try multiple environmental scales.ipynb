{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f843ec7b-9a08-4026-baf5-b60f8d4f05d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader, NeighborLoader, RandomNodeLoader, ClusterData, ClusterLoader\n",
    "from torch_geometric.nn import MessagePassing, SAGEConv\n",
    "from torch_geometric.utils import index_to_mask, to_undirected, remove_self_loops\n",
    "from torch_cluster import radius_graph\n",
    "from torch_scatter import scatter_add\n",
    "\n",
    "from data import *\n",
    "from loader import *\n",
    "\n",
    "import tqdm\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "from model import EdgeInteractionGNN, EdgeInteractionLayer, MultiSAGENet, SAGEGraphConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115ee631-b171-4e44-952e-3f57994f62b0",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c0557b1-ff9b-400c-8d61-246f36fba04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spatial_train_valid_indices(data, k: int, K: int = 3, boxsize: float = 75/0.6774, \n",
    "                                   pad: float = 3, indices_mask: Optional[torch.Tensor]=None):\n",
    "    \"\"\"Create spatial train/validation indices using z-coordinate splits.\n",
    "    \n",
    "    This creates spatially separated train/validation sets by dividing the simulation\n",
    "    box along the z-axis. It correctly handles periodic boundaries and optional\n",
    "    pre-filtering to create a true partition of the data.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_indices, valid_indices) as GLOBAL torch tensors, valid\n",
    "        for indexing the original `data` object.\n",
    "    \"\"\"\n",
    "\n",
    "    z_coords = data.pos[:, 2]\n",
    "\n",
    "    valid_start = (k / K * boxsize)\n",
    "    valid_end = ((k + 1) / K * boxsize)\n",
    "    \n",
    "    if k == K - 1:\n",
    "        spatial_valid_mask = (z_coords >= valid_start) | (z_coords < (valid_end % boxsize))\n",
    "    else:\n",
    "        spatial_valid_mask = (z_coords >= valid_start) & (z_coords < valid_end)\n",
    "    \n",
    "    train_start = ((k + 1) / K * boxsize + pad) % boxsize\n",
    "    train_end = (k / K * boxsize - pad) % boxsize\n",
    "    \n",
    "    if train_start > train_end:\n",
    "        spatial_train_mask = (z_coords >= train_start) | (z_coords <= train_end)\n",
    "    else:\n",
    "        spatial_train_mask = (z_coords >= train_start) & (z_coords <= train_end)\n",
    "\n",
    "    if indices_mask is None:\n",
    "        final_mask = torch.ones_like(z_coords, dtype=torch.bool)\n",
    "    else:\n",
    "        final_mask = indices_mask\n",
    "\n",
    "    # Use logical AND to get the final masks for training and validation\n",
    "    final_valid_mask = spatial_valid_mask & final_mask\n",
    "    final_train_mask = spatial_train_mask & final_mask\n",
    "\n",
    "    valid_indices = final_valid_mask.nonzero(as_tuple=True)[0]\n",
    "    train_indices = final_train_mask.nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # Double-check for overlap, which should now be impossible by construction\n",
    "    overlap = set(train_indices.tolist()) & set(valid_indices.tolist())\n",
    "    assert len(overlap) == 0, f\"Found {len(overlap)} overlapping indices\"\n",
    "\n",
    "    print(f\"Fold {k}/{K}: Train={len(train_indices)}, Valid={len(valid_indices)}\")\n",
    "    \n",
    "    return train_indices, valid_indices\n",
    "\n",
    "\n",
    "def gaussian_nll_loss(y_pred: torch.Tensor, y_true: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute Gaussian negative log-likelihood loss *with masking out infinite values*.\n",
    "    \n",
    "    Args:\n",
    "        y_pred: Model predictions\n",
    "        y_true: Ground truth values  \n",
    "        logvar: Log variance predictions\n",
    "        \n",
    "    Returns:\n",
    "        Gaussian NLL loss\n",
    "    \"\"\"\n",
    "    finite_mask = (y_true > 0.) & (y_true.isfinite())\n",
    "    \n",
    "    if not finite_mask.any():\n",
    "        return torch.tensor(0.0, device=y_pred.device, requires_grad=True)\n",
    "    \n",
    "\n",
    "    y_pred_masked = y_pred[finite_mask]\n",
    "    y_true_masked = y_true[finite_mask]\n",
    "    mse_loss = F.mse_loss(y_pred_masked, y_true_masked)\n",
    "    \n",
    "    return 0.5 * (mse_loss / 10**logvar + logvar)\n",
    "\n",
    "\n",
    "def compute_rmse(preds, targs):\n",
    "    \"\"\"lil helper func\"\"\"\n",
    "    finite_mask = (targs > 0.) & (np.isfinite(targs))\n",
    "    y_pred_masked = preds[finite_mask]\n",
    "    y_true_masked = targs[finite_mask]\n",
    "    return np.mean((y_pred_masked - y_true_masked)**2)**0.5\n",
    "\n",
    "\n",
    "def configure_optimizer(model, lr, wd,):\n",
    "    \"\"\"Only apply weight decay to weights, but not to other\n",
    "    parameters like biases or LayerNorm. Based on minGPT version.\n",
    "    \"\"\"\n",
    "\n",
    "    decay, no_decay = set(), set()\n",
    "    yes_wd_modules = (nn.Linear, )\n",
    "    no_wd_modules = (nn.LayerNorm, )\n",
    "    for mn, m in model.named_modules():\n",
    "        for pn, p in m.named_parameters():\n",
    "            fpn = '%s.%s' % (mn, pn) if mn else pn\n",
    "            if pn.endswith('bias'):\n",
    "                no_decay.add(fpn)\n",
    "            elif pn.endswith('weight') and isinstance(m, yes_wd_modules):\n",
    "                decay.add(fpn)\n",
    "            elif pn.endswith('weight') and isinstance(m, no_wd_modules):\n",
    "                no_decay.add(fpn)\n",
    "    param_dict = {pn: p for pn, p in model.named_parameters()}\n",
    "\n",
    "    optim_groups = [\n",
    "        {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": wd},\n",
    "        {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.},\n",
    "    ]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        optim_groups, \n",
    "        lr=lr, \n",
    "    )\n",
    "\n",
    "    return optimizer\n",
    "    \n",
    "\n",
    "def train_epoch_env_gnn(\n",
    "    dataloader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: str,\n",
    "    augment: bool = True\n",
    ") -> float:\n",
    "    \"\"\"Train one epoch for GNN model.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: Data loader for training data (X, y tuples)\n",
    "        model: Model to train\n",
    "        optimizer: Optimizer\n",
    "        device: Device to train on\n",
    "        \n",
    "    Returns:\n",
    "        Average training loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_total = 0\n",
    "    \n",
    "    for data in (dataloader):\n",
    "        if augment: # add random noise\n",
    "            data_node_features_scatter = 3e-4 * torch.randn_like(data.x[:, :-1]) * torch.std(data.x[:, :-1], dim=0)\n",
    "            data.x[:, :-1] += data_node_features_scatter\n",
    "            assert not torch.isnan(data.x).any() \n",
    "            \n",
    "            if hasattr(data, \"edge_attr\") and data.edge_attr is not None:\n",
    "                data_edge_features_scatter = 3e-4 * torch.randn_like(data.edge_attr) * torch.std(data.edge_attr, dim=0)            \n",
    "                data.edge_attr += data_edge_features_scatter\n",
    "                assert not torch.isnan(data.edge_attr).any() \n",
    "\n",
    "        data.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        y_pred, logvar_pred = output.chunk(2, dim=1)\n",
    "        \n",
    "        assert not torch.isnan(y_pred).any() and not torch.isnan(logvar_pred).any()\n",
    "        \n",
    "        y_pred = y_pred.view(-1, data.y.shape[1] if len(data.y.shape) > 1 else 2)\n",
    "        logvar_pred = logvar_pred.mean()\n",
    "        \n",
    "        loss = gaussian_nll_loss(y_pred, data.y, logvar_pred)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        loss_total += loss.item()\n",
    "        \n",
    "    return loss_total / len(dataloader)\n",
    "\n",
    "\n",
    "def validate_env_gnn(\n",
    "    dataloader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    device: str\n",
    ") -> Tuple[float, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Validate GNN model.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: Validation data loader\n",
    "        model: Model to validate\n",
    "        device: Device to validate on\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (loss, predictions, targets)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "\n",
    "    # kinda janky but otherwise this seems impossible to track\n",
    "    subhalo_ids = []\n",
    "    is_central = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        with torch.no_grad():\n",
    "            data.to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            y_pred, logvar_pred = output.chunk(2, dim=1)\n",
    "            \n",
    "            y_pred = y_pred.view(-1, data.y.shape[1] if len(data.y.shape) > 1 else 2)\n",
    "            logvar_pred = logvar_pred.mean()\n",
    "            \n",
    "            loss = gaussian_nll_loss(y_pred, data.y, logvar_pred)\n",
    "            loss_total += loss.item()\n",
    "            \n",
    "            y_preds.append(y_pred.detach().cpu().numpy())\n",
    "            y_trues.append(data.y.detach().cpu().numpy())\n",
    "            subhalo_ids.append(data.subhalo_id.detach().cpu().numpy())\n",
    "            is_central.append(data.is_central.detach().cpu().numpy())\n",
    "    \n",
    "    y_preds = np.concatenate(y_preds, axis=0)\n",
    "    y_trues = np.concatenate(y_trues, axis=0)\n",
    "    subhalo_ids = np.concatenate(subhalo_ids, axis=0)\n",
    "    is_central = np.concatenate(is_central, axis=0).flatten()\n",
    "    \n",
    "    return loss_total / len(dataloader), y_preds, y_trues, subhalo_ids, is_central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad288d49-33a7-4b78-8e4b-b991e2b0ebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_DISTANCE_SCALES = [0.6, 1.0, 1.4, 1.8, 2.3]\n",
    "\n",
    "K_FOLDS = 3\n",
    "USE_LOOPS = False\n",
    "NUM_PARTS = 48\n",
    "\n",
    "N_EPOCHS = 500\n",
    "LEARNING_RATE = 1e-2\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "N_LAYERS = 1\n",
    "N_HIDDEN = 64\n",
    "N_LATENT = 16\n",
    "N_UNSHARED_LAYERS = 16\n",
    "AGGR_FUNC = \"multi\"\n",
    "\n",
    "device = \"cuda\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab9dc05-565e-4232-a276-41adc26269e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on distance scale 0.6\n",
      "Fold 0/3: Train=84952, Valid=39685\n",
      "Fold 1/3: Train=75945, Valid=49913\n",
      "Fold 2/3: Train=81916, Valid=43355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 0 Training:   0%|                                                     | 0/500 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# data loading & determine split\n",
    "subhalos = pd.read_parquet(\"../results/subhalos.parquet\")\n",
    "\n",
    "for D_LINK in ENV_DISTANCE_SCALES:\n",
    "    print(f\"Working on distance scale {D_LINK}\")\n",
    "    \n",
    "    env_data_fname = f\"../results/env/cosmic_graphs_{D_LINK}Mpc.pkl\"\n",
    "    if not os.path.exists(env_data_fname):\n",
    "        env_data = make_cosmic_graph(subhalos, D_LINK) \n",
    "        with open(env_data_fname, \"wb\") as f:\n",
    "            pickle.dump(env_data, f)\n",
    "    else:\n",
    "        with open(env_data_fname, \"rb\") as f:\n",
    "            env_data = pickle.load(f)\n",
    "    \n",
    "    train_valid_split = [\n",
    "        get_spatial_train_valid_indices(env_data, k=k, K=K_FOLDS)\n",
    "        for k in range(K_FOLDS)\n",
    "    ]\n",
    "    \n",
    "    assert sum(len(v) for t, v in train_valid_split) == env_data.y.shape[0]\n",
    "    \n",
    "    # remove self-loops\n",
    "    if not USE_LOOPS:\n",
    "        env_data.edge_index, env_data.edge_attr = remove_self_loops(env_data.edge_index, env_data.edge_attr)\n",
    "    \n",
    "    # keep these variables for later use\n",
    "    is_central = env_data.is_central\n",
    "    subhalo_ids = env_data.subhalo_id\n",
    "    \n",
    "    # create a global mask but we'll use it later! -- note somewhat janky implementation for each fold right outside the training loop\n",
    "    isfinite_mask = np.logical_and(\n",
    "        np.isfinite(env_data.x).all(axis=1),\n",
    "        np.isfinite(env_data.y).any(axis=1)\n",
    "    ).type(torch.bool)\n",
    "    \n",
    "    \n",
    "    # dynamically determine num features\n",
    "    node_features = env_data.x.shape[1]\n",
    "    edge_features = env_data.edge_attr.shape[1]\n",
    "    out_features = env_data.y.shape[1]\n",
    "    \n",
    "    for k in range(K_FOLDS):\n",
    "        log_file = f\"../results/env/logs/env_gnn_{D_LINK}Mpc_fold_{k}.txt\"\n",
    "        \n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(f\"epoch,train_loss,valid_loss,valid_RMSE\\n\")\n",
    "    \n",
    "        # train-valid split & dataloaders\n",
    "        train_indices, valid_indices = train_valid_split[k]\n",
    "        \n",
    "        train_data = ClusterData(\n",
    "            env_data.subgraph(train_indices), \n",
    "            num_parts=NUM_PARTS, \n",
    "            recursive=False,\n",
    "            log=False\n",
    "        )\n",
    "        train_loader = ClusterLoader(\n",
    "            train_data,\n",
    "            shuffle=True,\n",
    "            batch_size=1,\n",
    "        )\n",
    "        \n",
    "        valid_data = ClusterData(\n",
    "            env_data.subgraph(valid_indices), \n",
    "            num_parts=NUM_PARTS // 2, \n",
    "            recursive=False,\n",
    "            log=False\n",
    "        )\n",
    "        valid_loader = ClusterLoader(\n",
    "            valid_data,\n",
    "            shuffle=False, \n",
    "            batch_size=1,\n",
    "        )\n",
    "    \n",
    "        model = EdgeInteractionGNN(\n",
    "            node_features=node_features,\n",
    "            edge_features=edge_features, \n",
    "            n_layers=N_LAYERS, \n",
    "            hidden_channels=N_HIDDEN,\n",
    "            latent_channels=N_LATENT,\n",
    "            n_unshared_layers=N_UNSHARED_LAYERS,\n",
    "            n_out=out_features,\n",
    "            aggr=([\"sum\", \"max\", \"mean\"] if AGGR_FUNC == \"multi\" else AGGR_FUNC)\n",
    "        )\n",
    "        model.to(device);\n",
    "    \n",
    "        optimizer = configure_optimizer(model, LEARNING_RATE, WEIGHT_DECAY)\n",
    "    \n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        valid_rmses = []\n",
    "        \n",
    "        epoch_pbar = tqdm.tqdm(range(N_EPOCHS), desc=f\"Fold {k} Training\", leave=True)\n",
    "        for epoch in epoch_pbar:\n",
    "            if epoch == int(N_EPOCHS * 0.25):\n",
    "                optimizer = configure_optimizer(model, LEARNING_RATE/5, WEIGHT_DECAY)\n",
    "            elif epoch == (N_EPOCHS * 0.5):\n",
    "                optimizer = configure_optimizer(model, LEARNING_RATE/25, WEIGHT_DECAY)\n",
    "            elif epoch == (N_EPOCHS * 0.75):\n",
    "                optimizer = configure_optimizer(model, LEARNING_RATE/125, WEIGHT_DECAY)\n",
    "                \n",
    "            train_loss = train_epoch_env_gnn(train_loader, model, optimizer, device=\"cuda\")\n",
    "            valid_loss, preds, targs, subhalo_ids, is_central = validate_env_gnn(valid_loader, model, device=\"cuda\")\n",
    "        \n",
    "            valid_rmse = compute_rmse(preds, targs)\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(f\"{epoch:d},{train_loss:.6f},{valid_loss:.6f},{valid_rmse:.6f}\\n\")\n",
    "                \n",
    "            train_losses.append(train_loss)\n",
    "            valid_losses.append(valid_loss)\n",
    "            valid_rmses.append(valid_rmse)\n",
    "        \n",
    "            epoch_pbar.set_postfix({'valid_rmse': f'{valid_rmse:.4f}'})\n",
    "        \n",
    "        # save predictions\n",
    "        results_file = f\"../results/env/predictions/env_gnn_{D_LINK}Mpc_fold_{k}.parquet\"\n",
    "    \n",
    "        results_df = pd.DataFrame({\n",
    "            \"subhalo_id\": subhalo_ids,\n",
    "            \"log_Mstar_pred\": preds[:, 0],\n",
    "            \"log_Mstar_true\": targs[:, 0],\n",
    "            \"log_Mgas_pred\": preds[:, 1],\n",
    "            \"log_Mgas_true\": targs[:, 1],\n",
    "            \"is_central\": is_central,\n",
    "        }).set_index(\"subhalo_id\")\n",
    "    \n",
    "        results_df.to_parquet(results_file)\n",
    "    \n",
    "        # save model weights\n",
    "        model_file = f\"../results/env/models/env_gnn_{D_LINK}Mpc_fold_{k}.pth\"\n",
    "        torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eb7a90-baf7-4f62-b2e8-345f44768ea1",
   "metadata": {},
   "source": [
    "# Results from varying environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ceb599b0-9d31-4c98-868c-640c728710a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "for D_LINK in [0.6, 1.0, 1.4, 1.8, 2.3]:\n",
    "    \n",
    "    results_dict[D_LINK] = pd.concat(\n",
    "        [pd.read_parquet(f\"../results/env/predictions/env_gnn_{D_LINK}Mpc_fold_{k}.parquet\") for k in range(K_FOLDS)],\n",
    "        axis=0\n",
    "    )\n",
    "    # include k folds\n",
    "    results_dict[D_LINK]['k_fold'] = np.concatenate([\n",
    "        np.full(pd.read_parquet(f\"../results/env/predictions/env_gnn_{D_LINK}Mpc_fold_{k}.parquet\").shape[0], k) \n",
    "        for k in range(K_FOLDS)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e7fbbeb-dbe5-48fe-8d89-5ad559fbd156",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, root_mean_squared_error, median_absolute_error\n",
    "from easyquery import Query, QueryMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "390d6f26-af51-4bba-84a3-7b709ef7d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_mapping = {\n",
    "    r\"$R^2$\": lambda p, y: r2_score(y, p, sample_weight=np.isfinite(y.values).nonzero()[0]),\n",
    "    r\"RMSE\": lambda p, y: root_mean_squared_error(p, y, sample_weight=np.isfinite(y.values).nonzero()[0]),\n",
    "    r\"MAE\":lambda p, y: mean_absolute_error(p, y, sample_weight=np.isfinite(y.values).nonzero()[0]),\n",
    "    r\"NMAD\": lambda p, y: 1.4826 * median_absolute_error(p, y, sample_weight=np.isfinite(y.values).nonzero()[0]),\n",
    "    r\"Bias\": lambda p, y: np.average(p - y, weights=np.isfinite(y.values).nonzero()[0])\n",
    "}\n",
    "\n",
    "min_stellar_mass = 8.5\n",
    "q_env_gnn = Query(\"is_central == 1\", f\"log_Mstar_true > {min_stellar_mass}\", QueryMaker.isfinite(\"log_Mgas_true\"))\n",
    "# q_env_gnn = Query(f\"log_Mstar_true > {min_stellar_mass}\", QueryMaker.isfinite(\"log_Mgas_true\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "80d9ab96-f619-48c3-b38a-f40b09bc7408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "log_Mstar\n",
      "==========\n",
      " 0.6 Mpc   $R^2$: $0.9188 \\pm 0.0059$\n",
      " 1.0 Mpc   $R^2$: $0.9210 \\pm 0.0049$\n",
      " 1.4 Mpc   $R^2$: $0.9175 \\pm 0.0060$\n",
      " 1.8 Mpc   $R^2$: $0.9232 \\pm 0.0043$\n",
      " 2.3 Mpc   $R^2$: $0.9214 \\pm 0.0024$\n",
      " 0.6 Mpc    RMSE: $0.1439 \\pm 0.0065$\n",
      " 1.0 Mpc    RMSE: $0.1409 \\pm 0.0087$\n",
      " 1.4 Mpc    RMSE: $0.1435 \\pm 0.0060$\n",
      " 1.8 Mpc    RMSE: $0.1389 \\pm 0.0054$\n",
      " 2.3 Mpc    RMSE: $0.1410 \\pm 0.0065$\n",
      " 0.6 Mpc     MAE: $0.1067 \\pm 0.0031$\n",
      " 1.0 Mpc     MAE: $0.1053 \\pm 0.0055$\n",
      " 1.4 Mpc     MAE: $0.1057 \\pm 0.0056$\n",
      " 1.8 Mpc     MAE: $0.1035 \\pm 0.0035$\n",
      " 2.3 Mpc     MAE: $0.1053 \\pm 0.0049$\n",
      " 0.6 Mpc    NMAD: $0.1236 \\pm 0.0032$\n",
      " 1.0 Mpc    NMAD: $0.1234 \\pm 0.0050$\n",
      " 1.4 Mpc    NMAD: $0.1209 \\pm 0.0100$\n",
      " 1.8 Mpc    NMAD: $0.1210 \\pm 0.0057$\n",
      " 2.3 Mpc    NMAD: $0.1225 \\pm 0.0058$\n",
      " 0.6 Mpc    Bias: $-0.0277 \\pm 0.0094$\n",
      " 1.0 Mpc    Bias: $-0.0285 \\pm 0.0131$\n",
      " 1.4 Mpc    Bias: $-0.0311 \\pm 0.0050$\n",
      " 1.8 Mpc    Bias: $-0.0263 \\pm 0.0054$\n",
      " 2.3 Mpc    Bias: $-0.0224 \\pm 0.0185$\n",
      "==========\n",
      "log_Mgas\n",
      "==========\n",
      " 0.6 Mpc   $R^2$: $0.8043 \\pm 0.0069$\n",
      " 1.0 Mpc   $R^2$: $0.8049 \\pm 0.0178$\n",
      " 1.4 Mpc   $R^2$: $0.8022 \\pm 0.0094$\n",
      " 1.8 Mpc   $R^2$: $0.8166 \\pm 0.0121$\n",
      " 2.3 Mpc   $R^2$: $0.8069 \\pm 0.0120$\n",
      " 0.6 Mpc    RMSE: $0.2169 \\pm 0.0109$\n",
      " 1.0 Mpc    RMSE: $0.2097 \\pm 0.0113$\n",
      " 1.4 Mpc    RMSE: $0.2157 \\pm 0.0210$\n",
      " 1.8 Mpc    RMSE: $0.2050 \\pm 0.0144$\n",
      " 2.3 Mpc    RMSE: $0.2158 \\pm 0.0182$\n",
      " 0.6 Mpc     MAE: $0.1350 \\pm 0.0107$\n",
      " 1.0 Mpc     MAE: $0.1313 \\pm 0.0069$\n",
      " 1.4 Mpc     MAE: $0.1328 \\pm 0.0139$\n",
      " 1.8 Mpc     MAE: $0.1295 \\pm 0.0068$\n",
      " 2.3 Mpc     MAE: $0.1387 \\pm 0.0117$\n",
      " 0.6 Mpc    NMAD: $0.1245 \\pm 0.0109$\n",
      " 1.0 Mpc    NMAD: $0.1224 \\pm 0.0071$\n",
      " 1.4 Mpc    NMAD: $0.1178 \\pm 0.0119$\n",
      " 1.8 Mpc    NMAD: $0.1201 \\pm 0.0056$\n",
      " 2.3 Mpc    NMAD: $0.1290 \\pm 0.0140$\n",
      " 0.6 Mpc    Bias: $-0.0093 \\pm 0.0040$\n",
      " 1.0 Mpc    Bias: $-0.0009 \\pm 0.0262$\n",
      " 1.4 Mpc    Bias: $0.0047 \\pm 0.0157$\n",
      " 1.8 Mpc    Bias: $-0.0073 \\pm 0.0119$\n",
      " 2.3 Mpc    Bias: $-0.0153 \\pm 0.0177$\n"
     ]
    }
   ],
   "source": [
    "# avg & error -- weighted by number of samples\n",
    "for target in [\"log_Mstar\", \"log_Mgas\"]:\n",
    "    print(\"\".join([\"=\"]*10))\n",
    "    print(f\"{target}\")\n",
    "    print(\"\".join([\"=\"]*10))\n",
    "    q = q_env_gnn\n",
    "\n",
    "    for metric, func in metrics_mapping.items():\n",
    "        for D_LINK in [0.6, 1.0, 1.4, 1.8, 2.3]:\n",
    "            df = results_dict[D_LINK]\n",
    "            scores = []\n",
    "            weights = []\n",
    "            for k in range(3):\n",
    "                qk = Query(f\"k_fold == {k}\")\n",
    "                filtered = (qk & q).filter(df)\n",
    "                scores.append(func(filtered[f\"{target}_pred\"], filtered[f\"{target}_true\"]))\n",
    "                weights.append(len(filtered))   \n",
    "            avg_weighted = np.average(scores, weights=weights)\n",
    "            std_weighted = np.sqrt(np.cov(scores, aweights=weights))\n",
    "\n",
    "            print(f\"{D_LINK: 0.1f} Mpc {metric: >7s}: ${avg_weighted:.4f} \\pm {std_weighted:.4f}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220dda6a-8197-439a-8be0-8bbdfcecea3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae146efd-5dbf-4d9d-b260-59b29a3d980d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyg]",
   "language": "python",
   "name": "conda-env-pyg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
