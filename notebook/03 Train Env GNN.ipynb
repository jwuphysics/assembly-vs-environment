{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f843ec7b-9a08-4026-baf5-b60f8d4f05d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader, NeighborLoader, RandomNodeLoader, ClusterData, ClusterLoader\n",
    "from torch_geometric.nn import MessagePassing, SAGEConv\n",
    "from torch_geometric.utils import index_to_mask, to_undirected, remove_self_loops\n",
    "from torch_cluster import radius_graph\n",
    "from torch_scatter import scatter_add\n",
    "\n",
    "import tqdm\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "from model import EdgeInteractionGNN, EdgeInteractionLayer, MultiSAGENet, SAGEGraphConvNet\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(\"../\").resolve()\n",
    "RESULTS_DIR = BASE_DIR / \"results\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffaeeac-bbee-49b8-b6ed-116595620034",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87039369-8b2f-4fb1-8a47-2ae5d151ec75",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Helper functions from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1416b2f-b87f-4b95-8312-9f40cccf7685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spatial_train_valid_indices(data, k: int, K: int = 3, boxsize: float = 75/0.6774, \n",
    "                                   pad: float = 3, indices_mask: Optional[torch.Tensor]=None):\n",
    "    \"\"\"Create spatial train/validation indices using z-coordinate splits.\n",
    "    \n",
    "    This creates spatially separated train/validation sets by dividing the simulation\n",
    "    box along the z-axis. It correctly handles periodic boundaries and optional\n",
    "    pre-filtering to create a true partition of the data.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_indices, valid_indices) as GLOBAL torch tensors, valid\n",
    "        for indexing the original `data` object.\n",
    "    \"\"\"\n",
    "\n",
    "    z_coords = data.pos[:, 2]\n",
    "\n",
    "    valid_start = (k / K * boxsize)\n",
    "    valid_end = ((k + 1) / K * boxsize)\n",
    "    \n",
    "    if k == K - 1:\n",
    "        spatial_valid_mask = (z_coords >= valid_start) | (z_coords < (valid_end % boxsize))\n",
    "    else:\n",
    "        spatial_valid_mask = (z_coords >= valid_start) & (z_coords < valid_end)\n",
    "    \n",
    "    train_start = ((k + 1) / K * boxsize + pad) % boxsize\n",
    "    train_end = (k / K * boxsize - pad) % boxsize\n",
    "    \n",
    "    if train_start > train_end:\n",
    "        spatial_train_mask = (z_coords >= train_start) | (z_coords <= train_end)\n",
    "    else:\n",
    "        spatial_train_mask = (z_coords >= train_start) & (z_coords <= train_end)\n",
    "\n",
    "    if indices_mask is None:\n",
    "        final_mask = torch.ones_like(z_coords, dtype=torch.bool)\n",
    "    else:\n",
    "        final_mask = indices_mask\n",
    "\n",
    "    # Use logical AND to get the final masks for training and validation\n",
    "    final_valid_mask = spatial_valid_mask & final_mask\n",
    "    final_train_mask = spatial_train_mask & final_mask\n",
    "\n",
    "    valid_indices = final_valid_mask.nonzero(as_tuple=True)[0]\n",
    "    train_indices = final_train_mask.nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # Double-check for overlap, which should now be impossible by construction\n",
    "    overlap = set(train_indices.tolist()) & set(valid_indices.tolist())\n",
    "    assert len(overlap) == 0, f\"Found {len(overlap)} overlapping indices\"\n",
    "\n",
    "    print(f\"Fold {k}/{K}: Train={len(train_indices)}, Valid={len(valid_indices)}\")\n",
    "    \n",
    "    return train_indices, valid_indices\n",
    "\n",
    "\n",
    "def gaussian_nll_loss(y_pred: torch.Tensor, y_true: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute Gaussian negative log-likelihood loss *with masking out infinite values*.\n",
    "    \n",
    "    Args:\n",
    "        y_pred: Model predictions\n",
    "        y_true: Ground truth values  \n",
    "        logvar: Log variance predictions\n",
    "        \n",
    "    Returns:\n",
    "        Gaussian NLL loss\n",
    "    \"\"\"\n",
    "    finite_mask = (y_true > 0.) & (y_true.isfinite())\n",
    "    \n",
    "    if not finite_mask.any():\n",
    "        return torch.tensor(0.0, device=y_pred.device, requires_grad=True)\n",
    "    \n",
    "\n",
    "    y_pred_masked = y_pred[finite_mask]\n",
    "    y_true_masked = y_true[finite_mask]\n",
    "    mse_loss = F.mse_loss(y_pred_masked, y_true_masked)\n",
    "    \n",
    "    return 0.5 * (mse_loss / 10**logvar + logvar)\n",
    "\n",
    "\n",
    "def compute_rmse(preds, targs):\n",
    "    \"\"\"lil helper func\"\"\"\n",
    "    finite_mask = (targs > 0.) & (np.isfinite(targs))\n",
    "    y_pred_masked = preds[finite_mask]\n",
    "    y_true_masked = targs[finite_mask]\n",
    "    return np.mean((y_pred_masked - y_true_masked)**2)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4381c561-cf96-4054-bbe3-f4b1bae8beba",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19c8dcf2-ef86-4058-9c1b-d63a6fd2f238",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../results/cosmic_graphs_3Mpc.pkl\", \"rb\") as f:\n",
    "    env_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b149b9-78f2-4b4a-bd2b-f026529c7f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_FOLDS = 3\n",
    "\n",
    "train_valid_split = [\n",
    "    get_spatial_train_valid_indices(env_data, k=k, K=K_FOLDS)\n",
    "    for k in range(K_FOLDS)\n",
    "]\n",
    "\n",
    "assert sum(len(v) for t, v in train_valid_split) == env_data.x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b6350e-893d-41c1-bd53-5b2809e85a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_LOOPS = False\n",
    "\n",
    "# remove self-loops\n",
    "if not USE_LOOPS:\n",
    "    env_data.edge_index, env_data.edge_attr = remove_self_loops(env_data.edge_index, env_data.edge_attr)\n",
    "\n",
    "# training\n",
    "node_features = env_data.x.shape[1]\n",
    "edge_features = env_data.edge_attr.shape[1]\n",
    "out_features = env_data.y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d450245-5d92-4c46-9c5a-30ea43a2c202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([132953, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_data.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8603fd0d-7c21-4b6b-b1d4-02f8154e3ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LAYERS = 1\n",
    "N_HIDDEN = 64\n",
    "N_LATENT = 16\n",
    "N_UNSHARED_LAYERS = 16\n",
    "AGGR_FUNC = \"multi\"\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model = EdgeInteractionGNN(\n",
    "    node_features=node_features,\n",
    "    edge_features=edge_features, \n",
    "    n_layers=N_LAYERS, \n",
    "    hidden_channels=N_HIDDEN,\n",
    "    latent_channels=N_LATENT,\n",
    "    n_unshared_layers=N_UNSHARED_LAYERS,\n",
    "    n_out=out_features,\n",
    "    aggr=([\"sum\", \"max\", \"mean\"] if AGGR_FUNC == \"multi\" else AGGR_FUNC)\n",
    ")\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a50d39c9-5c56-4f69-b598-561944e0eba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizer(model, lr, wd,):\n",
    "    \"\"\"Only apply weight decay to weights, but not to other\n",
    "    parameters like biases or LayerNorm. Based on minGPT version.\n",
    "    \"\"\"\n",
    "\n",
    "    decay, no_decay = set(), set()\n",
    "    yes_wd_modules = (nn.Linear, )\n",
    "    no_wd_modules = (nn.LayerNorm, )\n",
    "    for mn, m in model.named_modules():\n",
    "        for pn, p in m.named_parameters():\n",
    "            fpn = '%s.%s' % (mn, pn) if mn else pn\n",
    "            if pn.endswith('bias'):\n",
    "                no_decay.add(fpn)\n",
    "            elif pn.endswith('weight') and isinstance(m, yes_wd_modules):\n",
    "                decay.add(fpn)\n",
    "            elif pn.endswith('weight') and isinstance(m, no_wd_modules):\n",
    "                no_decay.add(fpn)\n",
    "    param_dict = {pn: p for pn, p in model.named_parameters()}\n",
    "\n",
    "    optim_groups = [\n",
    "        {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": wd},\n",
    "        {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.},\n",
    "    ]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        optim_groups, \n",
    "        lr=lr, \n",
    "    )\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c31d1da7-0c53-4420-b5cb-137b8fa45e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-2\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "optimizer = configure_optimizer(model, LEARNING_RATE, WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b8f71783-c6f0-4bfb-8d8d-c8d3e599da86",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, valid_indices = train_valid_split[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2659d573-e5c5-43cf-b805-e774d002faf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PARTS = 48\n",
    "\n",
    "train_data = ClusterData(\n",
    "    env_data.subgraph(train_indices), \n",
    "    num_parts=NUM_PARTS, \n",
    "    recursive=False,\n",
    "    log=False\n",
    ")\n",
    "train_loader = ClusterLoader(\n",
    "    train_data,\n",
    "    shuffle=True,\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "valid_data = ClusterData(\n",
    "    env_data.subgraph(valid_indices), \n",
    "    num_parts=NUM_PARTS // 2, \n",
    "    recursive=False,\n",
    "    log=False\n",
    ")\n",
    "valid_loader = ClusterLoader(\n",
    "    valid_data,\n",
    "    shuffle=True, \n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "84dc0986-b26a-4853-aeb3-b94cbf4364ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_env_gnn(\n",
    "    dataloader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: str,\n",
    "    augment: bool = True\n",
    ") -> float:\n",
    "    \"\"\"Train one epoch for GNN model.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: Data loader for training data (X, y tuples)\n",
    "        model: Model to train\n",
    "        optimizer: Optimizer\n",
    "        device: Device to train on\n",
    "        \n",
    "    Returns:\n",
    "        Average training loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_total = 0\n",
    "    \n",
    "    for data in (dataloader):\n",
    "        if augment: # add random noise\n",
    "            data_node_features_scatter = 3e-4 * torch.randn_like(data.x[:, :-1]) * torch.std(data.x[:, :-1], dim=0)\n",
    "            data.x[:, :-1] += data_node_features_scatter\n",
    "            assert not torch.isnan(data.x).any() \n",
    "            \n",
    "            if hasattr(data, \"edge_attr\") and data.edge_attr is not None:\n",
    "                data_edge_features_scatter = 3e-4 * torch.randn_like(data.edge_attr) * torch.std(data.edge_attr, dim=0)            \n",
    "                data.edge_attr += data_edge_features_scatter\n",
    "                assert not torch.isnan(data.edge_attr).any() \n",
    "\n",
    "        data.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        y_pred, logvar_pred = output.chunk(2, dim=1)\n",
    "        \n",
    "        assert not torch.isnan(y_pred).any() and not torch.isnan(logvar_pred).any()\n",
    "        \n",
    "        y_pred = y_pred.view(-1, data.y.shape[1] if len(data.y.shape) > 1 else 2)\n",
    "        logvar_pred = logvar_pred.mean()\n",
    "        \n",
    "        loss = gaussian_nll_loss(y_pred, data.y, logvar_pred)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        loss_total += loss.item()\n",
    "        \n",
    "    return loss_total / len(dataloader)\n",
    "\n",
    "\n",
    "def validate_env_gnn(\n",
    "    dataloader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    device: str\n",
    ") -> Tuple[float, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Validate GNN model.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: Validation data loader\n",
    "        model: Model to validate\n",
    "        device: Device to validate on\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (loss, predictions, targets)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        with torch.no_grad():\n",
    "            data.to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            y_pred, logvar_pred = output.chunk(2, dim=1)\n",
    "            \n",
    "            y_pred = y_pred.view(-1, data.y.shape[1] if len(data.y.shape) > 1 else 2)\n",
    "            logvar_pred = logvar_pred.mean()\n",
    "            \n",
    "            loss = gaussian_nll_loss(y_pred, data.y, logvar_pred)\n",
    "            loss_total += loss.item()\n",
    "            \n",
    "            y_preds.append(y_pred.detach().cpu().numpy())\n",
    "            y_trues.append(data.y.detach().cpu().numpy())\n",
    "    \n",
    "    y_preds = np.concatenate(y_preds, axis=0)\n",
    "    y_trues = np.concatenate(y_trues, axis=0)\n",
    "    \n",
    "    return loss_total / len(dataloader), y_preds, y_trues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c0452772-55fa-42e1-985c-e46fffe6d6ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9 0.9628\n",
      " 19 0.9885\n",
      " 29 0.9885\n",
      " 39 0.9910\n",
      " 49 0.6802\n",
      " 59 0.6597\n",
      " 69 0.6333\n",
      " 79 0.3510\n",
      " 89 0.3662\n",
      " 99 0.3395\n",
      "109 0.3232\n",
      "119 0.3469\n",
      "129 0.3223\n",
      "139 0.3121\n",
      "149 0.3091\n",
      "159 0.3131\n",
      "169 0.3043\n",
      "179 0.2993\n",
      "189 0.3069\n",
      "199 0.3159\n",
      "209 0.2933\n",
      "219 0.2940\n",
      "229 0.3033\n",
      "239 0.2916\n",
      "249 0.3007\n",
      "259 0.2891\n",
      "269 0.2910\n",
      "279 0.2889\n",
      "289 0.2907\n",
      "299 0.2889\n",
      "309 0.2882\n",
      "319 0.2896\n",
      "329 0.2884\n",
      "339 0.2884\n",
      "349 0.2881\n",
      "359 0.2887\n",
      "369 0.2887\n",
      "379 0.2881\n",
      "389 0.2883\n",
      "399 0.2879\n",
      "409 0.2882\n",
      "419 0.2879\n",
      "429 0.2879\n",
      "439 0.2877\n",
      "449 0.2878\n",
      "459 0.2879\n",
      "469 0.2880\n",
      "479 0.2879\n",
      "489 0.2878\n",
      "499 0.2880\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 500\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "valid_rmses = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    if epoch == int(N_EPOCHS * 0.25):\n",
    "        optimizer = configure_optimizer(model, LEARNING_RATE/5, WEIGHT_DECAY)\n",
    "    elif epoch == (N_EPOCHS * 0.5):\n",
    "        optimizer = configure_optimizer(model, LEARNING_RATE/25, WEIGHT_DECAY)\n",
    "    elif epoch == (N_EPOCHS * 0.75):\n",
    "        optimizer = configure_optimizer(model, LEARNING_RATE/125, WEIGHT_DECAY)\n",
    "        \n",
    "    train_loss = train_epoch_env_gnn(train_loader, model, optimizer, device=\"cuda\")\n",
    "    valid_loss, preds, targs = validate_env_gnn(valid_loader, model, device=\"cuda\")\n",
    "\n",
    "    valid_rmse = compute_rmse(preds, targs)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_rmses.append(valid_rmse)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"{epoch: >3d} {valid_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0d3da7-7439-458d-b107-9ad7fb745e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4), dpi=150)\n",
    "select_centrals = (env_data.subgraph(valid_indices).is_central).flatten().numpy().astype(bool)\n",
    "\n",
    "plt.scatter(targs[:, 0][select_centrals], preds[:, 0][select_centrals], c=\"C3\", s=3, edgecolor=\"none\", alpha=0.5, )\n",
    "plt.scatter(targs[:, 0][~select_centrals], preds[:, 0][~select_centrals], c=\"C0\", s=3, edgecolor=\"none\", alpha=0.5, )\n",
    "plt.plot([8, 12], [8, 12], ls=\"-\", c=\"k\", lw=1) \n",
    "plt.xlim(8, 12)\n",
    "plt.ylim(8, 12)\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.grid(alpha=0.15)\n",
    "plt.xlabel(r\"True log($M_{\\bigstar}/M_{\\odot}$)\", fontsize=12)\n",
    "plt.ylabel(r\"Predicted log($M_{\\bigstar}/M_{\\odot}$)\", fontsize=12);\n",
    "\n",
    "plt.clf();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115ee631-b171-4e44-952e-3f57994f62b0",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c0557b1-ff9b-400c-8d61-246f36fba04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spatial_train_valid_indices(data, k: int, K: int = 3, boxsize: float = 75/0.6774, \n",
    "                                   pad: float = 3, indices_mask: Optional[torch.Tensor]=None):\n",
    "    \"\"\"Create spatial train/validation indices using z-coordinate splits.\n",
    "    \n",
    "    This creates spatially separated train/validation sets by dividing the simulation\n",
    "    box along the z-axis. It correctly handles periodic boundaries and optional\n",
    "    pre-filtering to create a true partition of the data.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_indices, valid_indices) as GLOBAL torch tensors, valid\n",
    "        for indexing the original `data` object.\n",
    "    \"\"\"\n",
    "\n",
    "    z_coords = data.pos[:, 2]\n",
    "\n",
    "    valid_start = (k / K * boxsize)\n",
    "    valid_end = ((k + 1) / K * boxsize)\n",
    "    \n",
    "    if k == K - 1:\n",
    "        spatial_valid_mask = (z_coords >= valid_start) | (z_coords < (valid_end % boxsize))\n",
    "    else:\n",
    "        spatial_valid_mask = (z_coords >= valid_start) & (z_coords < valid_end)\n",
    "    \n",
    "    train_start = ((k + 1) / K * boxsize + pad) % boxsize\n",
    "    train_end = (k / K * boxsize - pad) % boxsize\n",
    "    \n",
    "    if train_start > train_end:\n",
    "        spatial_train_mask = (z_coords >= train_start) | (z_coords <= train_end)\n",
    "    else:\n",
    "        spatial_train_mask = (z_coords >= train_start) & (z_coords <= train_end)\n",
    "\n",
    "    if indices_mask is None:\n",
    "        final_mask = torch.ones_like(z_coords, dtype=torch.bool)\n",
    "    else:\n",
    "        final_mask = indices_mask\n",
    "\n",
    "    # Use logical AND to get the final masks for training and validation\n",
    "    final_valid_mask = spatial_valid_mask & final_mask\n",
    "    final_train_mask = spatial_train_mask & final_mask\n",
    "\n",
    "    valid_indices = final_valid_mask.nonzero(as_tuple=True)[0]\n",
    "    train_indices = final_train_mask.nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # Double-check for overlap, which should now be impossible by construction\n",
    "    overlap = set(train_indices.tolist()) & set(valid_indices.tolist())\n",
    "    assert len(overlap) == 0, f\"Found {len(overlap)} overlapping indices\"\n",
    "\n",
    "    print(f\"Fold {k}/{K}: Train={len(train_indices)}, Valid={len(valid_indices)}\")\n",
    "    \n",
    "    return train_indices, valid_indices\n",
    "\n",
    "\n",
    "def gaussian_nll_loss(y_pred: torch.Tensor, y_true: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute Gaussian negative log-likelihood loss *with masking out infinite values*.\n",
    "    \n",
    "    Args:\n",
    "        y_pred: Model predictions\n",
    "        y_true: Ground truth values  \n",
    "        logvar: Log variance predictions\n",
    "        \n",
    "    Returns:\n",
    "        Gaussian NLL loss\n",
    "    \"\"\"\n",
    "    finite_mask = (y_true > 0.) & (y_true.isfinite())\n",
    "    \n",
    "    if not finite_mask.any():\n",
    "        return torch.tensor(0.0, device=y_pred.device, requires_grad=True)\n",
    "    \n",
    "\n",
    "    y_pred_masked = y_pred[finite_mask]\n",
    "    y_true_masked = y_true[finite_mask]\n",
    "    mse_loss = F.mse_loss(y_pred_masked, y_true_masked)\n",
    "    \n",
    "    return 0.5 * (mse_loss / 10**logvar + logvar)\n",
    "\n",
    "\n",
    "def compute_rmse(preds, targs):\n",
    "    \"\"\"lil helper func\"\"\"\n",
    "    finite_mask = (targs > 0.) & (np.isfinite(targs))\n",
    "    y_pred_masked = preds[finite_mask]\n",
    "    y_true_masked = targs[finite_mask]\n",
    "    return np.mean((y_pred_masked - y_true_masked)**2)**0.5\n",
    "\n",
    "\n",
    "def configure_optimizer(model, lr, wd,):\n",
    "    \"\"\"Only apply weight decay to weights, but not to other\n",
    "    parameters like biases or LayerNorm. Based on minGPT version.\n",
    "    \"\"\"\n",
    "\n",
    "    decay, no_decay = set(), set()\n",
    "    yes_wd_modules = (nn.Linear, )\n",
    "    no_wd_modules = (nn.LayerNorm, )\n",
    "    for mn, m in model.named_modules():\n",
    "        for pn, p in m.named_parameters():\n",
    "            fpn = '%s.%s' % (mn, pn) if mn else pn\n",
    "            if pn.endswith('bias'):\n",
    "                no_decay.add(fpn)\n",
    "            elif pn.endswith('weight') and isinstance(m, yes_wd_modules):\n",
    "                decay.add(fpn)\n",
    "            elif pn.endswith('weight') and isinstance(m, no_wd_modules):\n",
    "                no_decay.add(fpn)\n",
    "    param_dict = {pn: p for pn, p in model.named_parameters()}\n",
    "\n",
    "    optim_groups = [\n",
    "        {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": wd},\n",
    "        {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.},\n",
    "    ]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        optim_groups, \n",
    "        lr=lr, \n",
    "    )\n",
    "\n",
    "    return optimizer\n",
    "    \n",
    "\n",
    "def train_epoch_env_gnn(\n",
    "    dataloader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: str,\n",
    "    augment: bool = True\n",
    ") -> float:\n",
    "    \"\"\"Train one epoch for GNN model.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: Data loader for training data (X, y tuples)\n",
    "        model: Model to train\n",
    "        optimizer: Optimizer\n",
    "        device: Device to train on\n",
    "        \n",
    "    Returns:\n",
    "        Average training loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_total = 0\n",
    "    \n",
    "    for data in (dataloader):\n",
    "        if augment: # add random noise\n",
    "            data_node_features_scatter = 3e-4 * torch.randn_like(data.x[:, :-1]) * torch.std(data.x[:, :-1], dim=0)\n",
    "            data.x[:, :-1] += data_node_features_scatter\n",
    "            assert not torch.isnan(data.x).any() \n",
    "            \n",
    "            if hasattr(data, \"edge_attr\") and data.edge_attr is not None:\n",
    "                data_edge_features_scatter = 3e-4 * torch.randn_like(data.edge_attr) * torch.std(data.edge_attr, dim=0)            \n",
    "                data.edge_attr += data_edge_features_scatter\n",
    "                assert not torch.isnan(data.edge_attr).any() \n",
    "\n",
    "        data.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        y_pred, logvar_pred = output.chunk(2, dim=1)\n",
    "        \n",
    "        assert not torch.isnan(y_pred).any() and not torch.isnan(logvar_pred).any()\n",
    "        \n",
    "        y_pred = y_pred.view(-1, data.y.shape[1] if len(data.y.shape) > 1 else 2)\n",
    "        logvar_pred = logvar_pred.mean()\n",
    "        \n",
    "        loss = gaussian_nll_loss(y_pred, data.y, logvar_pred)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        loss_total += loss.item()\n",
    "        \n",
    "    return loss_total / len(dataloader)\n",
    "\n",
    "\n",
    "def validate_env_gnn(\n",
    "    dataloader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    device: str\n",
    ") -> Tuple[float, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Validate GNN model.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: Validation data loader\n",
    "        model: Model to validate\n",
    "        device: Device to validate on\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (loss, predictions, targets)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "\n",
    "    # kinda janky but otherwise this seems impossible to track\n",
    "    subhalo_ids = []\n",
    "    is_central = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        with torch.no_grad():\n",
    "            data.to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            y_pred, logvar_pred = output.chunk(2, dim=1)\n",
    "            \n",
    "            y_pred = y_pred.view(-1, data.y.shape[1] if len(data.y.shape) > 1 else 2)\n",
    "            logvar_pred = logvar_pred.mean()\n",
    "            \n",
    "            loss = gaussian_nll_loss(y_pred, data.y, logvar_pred)\n",
    "            loss_total += loss.item()\n",
    "            \n",
    "            y_preds.append(y_pred.detach().cpu().numpy())\n",
    "            y_trues.append(data.y.detach().cpu().numpy())\n",
    "            subhalo_ids.append(data.subhalo_id.detach().cpu().numpy())\n",
    "            is_central.append(data.is_central.detach().cpu().numpy())\n",
    "    \n",
    "    y_preds = np.concatenate(y_preds, axis=0)\n",
    "    y_trues = np.concatenate(y_trues, axis=0)\n",
    "    subhalo_ids = np.concatenate(subhalo_ids, axis=0)\n",
    "    is_central = np.concatenate(is_central, axis=0).flatten()\n",
    "    \n",
    "    return loss_total / len(dataloader), y_preds, y_trues, subhalo_ids, is_central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad288d49-33a7-4b78-8e4b-b991e2b0ebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_FOLDS = 3\n",
    "USE_LOOPS = False\n",
    "NUM_PARTS = 48\n",
    "\n",
    "if \"results-mhalo-vmax\" in str(RESULTS_DIR):\n",
    "    N_EPOCHS = 500 # if including multiple features\n",
    "    N_HIDDEN = 64\n",
    "    N_LATENT = 16\n",
    "else:\n",
    "    N_EPOCHS = 300 # if including multiple features\n",
    "    N_HIDDEN = 128\n",
    "    N_LATENT = 32\n",
    "\n",
    "LEARNING_RATE = 1e-2\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "N_LAYERS = 1\n",
    "\n",
    "N_UNSHARED_LAYERS = 16\n",
    "AGGR_FUNC = \"multi\"\n",
    "\n",
    "device = \"cuda\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ab9dc05-565e-4232-a276-41adc26269e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0/3: Train=84952, Valid=39685\n",
      "Fold 1/3: Train=75945, Valid=49913\n",
      "Fold 2/3: Train=81916, Valid=43355\n"
     ]
    }
   ],
   "source": [
    "# data loading & determine split\n",
    "with open(RESULTS_DIR / \"cosmic_graphs_3Mpc.pkl\", \"rb\") as f:\n",
    "    env_data = pickle.load(f)\n",
    "\n",
    "train_valid_split = [\n",
    "    get_spatial_train_valid_indices(env_data, k=k, K=K_FOLDS)\n",
    "    for k in range(K_FOLDS)\n",
    "]\n",
    "\n",
    "assert sum(len(v) for t, v in train_valid_split) == env_data.y.shape[0]\n",
    "\n",
    "# remove self-loops\n",
    "if not USE_LOOPS:\n",
    "    env_data.edge_index, env_data.edge_attr = remove_self_loops(env_data.edge_index, env_data.edge_attr)\n",
    "\n",
    "# keep these variables for later use\n",
    "is_central = env_data.is_central\n",
    "subhalo_ids = env_data.subhalo_id\n",
    "\n",
    "# create a global mask but we'll use it later! -- note somewhat janky implementation for each fold right outside the training loop\n",
    "isfinite_mask = np.logical_and(\n",
    "    np.isfinite(env_data.x).all(axis=1),\n",
    "    np.isfinite(env_data.y).any(axis=1)\n",
    ").type(torch.bool)\n",
    "\n",
    "\n",
    "# dynamically determine num features\n",
    "node_features = env_data.x.shape[1]\n",
    "edge_features = env_data.edge_attr.shape[1]\n",
    "out_features = env_data.y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01384392-ce2d-4546-8848-48fef0a8a739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lazy imputing...\n",
    "env_data.x[~torch.isfinite(env_data.x)] = -10 # again -- only if including multiple features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1008dfc2-9f7b-4665-8c17-ad2584b4c2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 0 Training:  15%|███▊                     | 45/300 [02:01<11:25,  2.69s/it, valid_rmse=0.2579]"
     ]
    }
   ],
   "source": [
    "for k in range(K_FOLDS):\n",
    "    log_file = RESULTS_DIR / f\"logs/env_gnn_fold_{k}.txt\"\n",
    "    \n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(f\"epoch,train_loss,valid_loss,valid_RMSE\\n\")\n",
    "\n",
    "    # train-valid split & dataloaders\n",
    "    train_indices, valid_indices = train_valid_split[k]\n",
    "    \n",
    "    train_data = ClusterData(\n",
    "        env_data.subgraph(train_indices), \n",
    "        num_parts=NUM_PARTS, \n",
    "        recursive=False,\n",
    "        log=False\n",
    "    )\n",
    "    train_loader = ClusterLoader(\n",
    "        train_data,\n",
    "        shuffle=True,\n",
    "        batch_size=1,\n",
    "    )\n",
    "    \n",
    "    valid_data = ClusterData(\n",
    "        env_data.subgraph(valid_indices), \n",
    "        num_parts=NUM_PARTS // 2, \n",
    "        recursive=False,\n",
    "        log=False\n",
    "    )\n",
    "    valid_loader = ClusterLoader(\n",
    "        valid_data,\n",
    "        shuffle=False, \n",
    "        batch_size=1,\n",
    "    )\n",
    "\n",
    "    model = EdgeInteractionGNN(\n",
    "        node_features=node_features,\n",
    "        edge_features=edge_features, \n",
    "        n_layers=N_LAYERS, \n",
    "        hidden_channels=N_HIDDEN,\n",
    "        latent_channels=N_LATENT,\n",
    "        n_unshared_layers=N_UNSHARED_LAYERS,\n",
    "        n_out=out_features,\n",
    "        aggr=([\"sum\", \"max\", \"mean\"] if AGGR_FUNC == \"multi\" else AGGR_FUNC)\n",
    "    )\n",
    "    model.to(device);\n",
    "\n",
    "    optimizer = configure_optimizer(model, LEARNING_RATE, WEIGHT_DECAY)\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    valid_rmses = []\n",
    "    \n",
    "    epoch_pbar = tqdm.tqdm(range(N_EPOCHS), desc=f\"Fold {k} Training\", leave=True)\n",
    "    for epoch in epoch_pbar:\n",
    "        if epoch == int(N_EPOCHS * 0.25):\n",
    "            optimizer = configure_optimizer(model, LEARNING_RATE/5, WEIGHT_DECAY)\n",
    "        elif epoch == (N_EPOCHS * 0.5):\n",
    "            optimizer = configure_optimizer(model, LEARNING_RATE/25, WEIGHT_DECAY)\n",
    "        elif epoch == (N_EPOCHS * 0.75):\n",
    "            optimizer = configure_optimizer(model, LEARNING_RATE/125, WEIGHT_DECAY)\n",
    "            \n",
    "        train_loss = train_epoch_env_gnn(train_loader, model, optimizer, device=\"cuda\")\n",
    "        valid_loss, preds, targs, subhalo_ids, is_central = validate_env_gnn(valid_loader, model, device=\"cuda\")\n",
    "    \n",
    "        valid_rmse = compute_rmse(preds, targs)\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(f\"{epoch:d},{train_loss:.6f},{valid_loss:.6f},{valid_rmse:.6f}\\n\")\n",
    "            \n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_rmses.append(valid_rmse)\n",
    "    \n",
    "        epoch_pbar.set_postfix({'valid_rmse': f'{valid_rmse:.4f}'})\n",
    "    \n",
    "    # save predictions\n",
    "    results_file = RESULTS_DIR / f\"predictions/env_gnn_fold_{k}.parquet\"\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        \"subhalo_id\": subhalo_ids,\n",
    "        \"log_Mstar_pred\": preds[:, 0],\n",
    "        \"log_Mstar_true\": targs[:, 0],\n",
    "        \"log_Mgas_pred\": preds[:, 1],\n",
    "        \"log_Mgas_true\": targs[:, 1],\n",
    "        \"is_central\": is_central,\n",
    "    }).set_index(\"subhalo_id\")\n",
    "\n",
    "    results_df.to_parquet(results_file)\n",
    "\n",
    "    # save model weights\n",
    "    model_file = RESULTS_DIR / f\"models/env_gnn_fold_{k}.pth\"\n",
    "    torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d4023-8f3d-4c41-a76a-e6043c9bc844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyg]",
   "language": "python",
   "name": "conda-env-pyg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
