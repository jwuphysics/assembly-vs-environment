{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46eb5f20-71c6-42d3-b959-ba524a106378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.loader import DataLoader, DynamicBatchSampler\n",
    "from torch_geometric.nn import MessagePassing, SAGEConv, global_mean_pool, global_max_pool, global_add_pool\n",
    "from torch_geometric.utils import index_to_mask, to_undirected, remove_self_loops\n",
    "from torch_cluster import radius_graph\n",
    "from torch_scatter import scatter_add\n",
    "\n",
    "import tqdm\n",
    "from typing import Tuple, Optional, List\n",
    "\n",
    "from loader import create_bonsai_stump_pairs\n",
    "from model import EdgeInteractionGNN, EdgeInteractionLayer, MultiSAGENet, ModernSAGENet, SAGEEncoder, BonsaiStumpSAGENet, ModernBonsaiStumpSAGENet\n",
    "\n",
    "from muon import SingleDeviceMuonWithAuxAdam\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "BASE_DIR = Path(\"../\")\n",
    "RESULTS_DIR = Path(BASE_DIR / \"results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d2e790-749b-47d5-89fb-444c56992029",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bfc77a-77cb-4bcc-bc26-718208904be7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load previous helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6953f621-953d-4100-8b27-4611950f00f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spatial_train_valid_indices(data, k: int, K: int = 3, boxsize: float = 75/0.6774, \n",
    "                                   pad: float = 3, epsilon: float = 1e-10, indices_mask=None):\n",
    "    \"\"\"Create spatial train/validation indices using z-coordinate splits.\n",
    "    \n",
    "    This creates spatially separated train/validation sets by dividing the simulation\n",
    "    box along the z-axis. Each fold uses 1/K of the box for validation and the rest\n",
    "    for training (with padding to avoid boundary effects).\n",
    "    \n",
    "    Args:\n",
    "        data: PyTorch Geometric data object with pos attribute\n",
    "        k: Fold index (0 to K-1)\n",
    "        K: Total number of folds\n",
    "        boxsize: Simulation box size in Mpc\n",
    "        pad: Padding between train/valid regions in Mpc\n",
    "        epsilon: Small value to avoid boundary issues\n",
    "        indices_mask: either None or a boolean mask of length X.shape[0]\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (train_indices, valid_indices) as torch tensors\n",
    "    \"\"\"\n",
    "\n",
    "    if indices_mask is None:\n",
    "        z_coords = data.pos[:, 2]\n",
    "    else:\n",
    "        z_coords = data.pos[:, 2][indices_mask]\n",
    "    \n",
    "    \n",
    "    # Calculate validation region boundaries\n",
    "    valid_start = (k / K * boxsize) % boxsize\n",
    "    valid_end = ((k + 1) / K * boxsize) % boxsize\n",
    "    \n",
    "    # Handle wrap-around case\n",
    "    if valid_start > valid_end:  # Wraps around the boundary\n",
    "        valid_mask = (z_coords >= valid_start) | (z_coords <= valid_end)\n",
    "    else:\n",
    "        valid_mask = (z_coords >= valid_start) & (z_coords <= valid_end)\n",
    "    \n",
    "    # Create training region with padding\n",
    "    train_start = ((k + 1) / K * boxsize + pad) % boxsize\n",
    "    train_end = (k / K * boxsize - pad) % boxsize\n",
    "    \n",
    "    # Handle wrap-around for training region\n",
    "    if train_start > train_end:  # Wraps around the boundary\n",
    "        train_mask = (z_coords >= train_start) | (z_coords <= train_end)\n",
    "    else:\n",
    "        train_mask = (z_coords >= train_start) & (z_coords <= train_end)\n",
    "\n",
    "\n",
    "    # Get indices\n",
    "    train_indices = train_mask.nonzero(as_tuple=True)[0]\n",
    "    valid_indices = valid_mask.nonzero(as_tuple=True)[0]\n",
    "    \n",
    "    # Ensure zero overlap\n",
    "    overlap = set(train_indices.tolist()) & set(valid_indices.tolist())\n",
    "    assert len(overlap) == 0, f\"Found {len(overlap)} overlapping indices between train and validation\"\n",
    "    \n",
    "    print(f\"Fold {k}/{K}: Train={len(train_indices)}, Valid={len(valid_indices)}\")\n",
    "    \n",
    "    return train_indices, valid_indices\n",
    "\n",
    "\n",
    "def gaussian_nll_loss(y_pred: torch.Tensor, y_true: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute Gaussian negative log-likelihood loss *with masking out infinite values*.\n",
    "    \n",
    "    Args:\n",
    "        y_pred: Model predictions\n",
    "        y_true: Ground truth values  \n",
    "        logvar: Log variance predictions\n",
    "        \n",
    "    Returns:\n",
    "        Gaussian NLL loss\n",
    "    \"\"\"\n",
    "    finite_mask = (y_true > 0.) & (y_true.isfinite())\n",
    "    \n",
    "    if not finite_mask.any():\n",
    "        return torch.tensor(0.0, device=y_pred.device, requires_grad=True)\n",
    "    \n",
    "\n",
    "    y_pred_masked = y_pred[finite_mask]\n",
    "    y_true_masked = y_true[finite_mask]\n",
    "    mse_loss = F.mse_loss(y_pred_masked, y_true_masked)\n",
    "    \n",
    "    return 0.5 * (mse_loss / 10**logvar + logvar)\n",
    "\n",
    "\n",
    "def compute_rmse(preds, targs):\n",
    "    \"\"\"lil helper func\"\"\"\n",
    "    finite_mask = (targs > 0.) & (np.isfinite(targs))\n",
    "    y_pred_masked = preds[finite_mask]\n",
    "    y_true_masked = targs[finite_mask]\n",
    "    return np.mean((y_pred_masked - y_true_masked)**2)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954e73d3-16de-484f-8806-20049831adc3",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57b2fc02-cb47-4de0-9236-8c8d150bea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../results/merger_trees.pkl\", \"rb\") as f:\n",
    "    tree_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "76c4436b-e845-47e4-9a0f-051f50ccde52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[902888, 4], edge_index=[2, 902887], y=[2], root_subhalo_id=59551)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93d43d0b-a97e-4769-adbc-f7248ac2304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subhalos_per_tree = [tree.x.shape[0] for tree in tree_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777df365-ce98-440e-ba59-bde5623754f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4), dpi=150)\n",
    "plt.hist(np.log10(n_subhalos_per_tree), log=True, bins=100)\n",
    "plt.xlabel(r\"log $N_{\\rm subhalo\\ per\\ tree}$\", fontsize=12)\n",
    "plt.ylabel(r\"$dN_{\\rm trees}/d \\log N_{\\rm subhalo\\ per\\ tree}$\", fontsize=12);\n",
    "plt.grid(alpha=0.15)\n",
    "plt.clf();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32c14434-1026-4f22-9895-23268911d81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get positions to assign folds...\n",
    "with open(\"../results/cosmic_graphs_3Mpc.pkl\", \"rb\") as f:\n",
    "    env_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d218c8cf-ed5d-49d0-96a9-f2864331692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_subhalo_ids = [tree.root_subhalo_id for tree in tree_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf7ab7db-83f5-494b-9398-641bada32dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[132953, 3], edge_index=[2, 6653247], edge_attr=[6653247, 6], y=[132953, 2], pos=[132953, 3], vel=[132953, 3], is_central=[132953, 1], x_hydro=[132953, 2], pos_hydro=[132953, 3], vel_hydro=[132953, 3], halfmassradius=[132953, 1], subhalo_id=[132953], overdensity=[132953])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0c146f0-f225-4419-bfdc-148fa572d508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num merger trees: 123004\n",
      "Num env nodes: 132953\n",
      "Crossmatches: 123001\n"
     ]
    }
   ],
   "source": [
    "tree_crossmatches = torch.isin(env_data.subhalo_id, torch.tensor(tree_subhalo_ids))\n",
    "\n",
    "print(\"Num merger trees:\", len(tree_data))\n",
    "print(\"Num env nodes:\", env_data.x.shape[0])\n",
    "\n",
    "print(\"Crossmatches:\", tree_crossmatches.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc6c0f47-4148-446a-a4e5-92b35131625e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0/3: Train=78822, Valid=36452\n",
      "Fold 1/3: Train=70062, Valid=46397\n",
      "Fold 2/3: Train=75703, Valid=40152\n"
     ]
    }
   ],
   "source": [
    "K_FOLDS = 3\n",
    "\n",
    "train_valid_split = [\n",
    "    get_spatial_train_valid_indices(env_data, k=k, K=K_FOLDS, indices_mask=tree_crossmatches)\n",
    "    for k in range(K_FOLDS)\n",
    "]\n",
    "\n",
    "assert sum(len(v) for t, v in train_valid_split) == tree_crossmatches.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e162624-8ff0-4893-87ff-cffeaf7570e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from subhalo_id to tree for efficient lookup\n",
    "tree_map = {tree.root_subhalo_id: tree for tree in tree_data}\n",
    "\n",
    "# full list of subhalo IDs that have trees, in the env_data order\n",
    "subhalo_ids_with_trees = env_data.subhalo_id[tree_crossmatches]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f1b73-b058-41d6-8f66-948457e03cf9",
   "metadata": {},
   "source": [
    "## Sampling and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adf8cdab-74f1-4414-8b18-bee9472ac07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "\n",
    "train_indices, valid_indices = train_valid_split[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bc345b1c-6dfe-484b-bc5b-2886ecb8f37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the tree_data[0].y shape needs to be like [1, 2], otherwise it'll be flattened\n",
    "for tree in tree_data:\n",
    "    tree.y = tree.y.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "90845a77-939e-4278-811a-4dbc6187a05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# THIS IS WRONG!\n",
    "# train_dataset = [tree_data[idx] for idx in train_indices]\n",
    "# valid_dataset = [tree_data[idx] for idx in valid_indices]\n",
    "\n",
    "# correct subhalo_ids for the train/valid split\n",
    "train_ids_for_fold = subhalo_ids_with_trees[train_indices].numpy()\n",
    "valid_ids_for_fold = subhalo_ids_with_trees[valid_indices].numpy()\n",
    "\n",
    "# Build datasets by looking up subhalo_id\n",
    "train_dataset = [tree_map[sub_id] for sub_id in train_ids_for_fold]\n",
    "valid_dataset = [tree_map[sub_id] for sub_id in valid_ids_for_fold]\n",
    "\n",
    "train_sampler = DynamicBatchSampler(\n",
    "    train_dataset,\n",
    "    max_num=max(n_subhalos_per_tree), # around 1e6\n",
    "    mode=\"node\",\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "valid_sampler = DynamicBatchSampler(\n",
    "    valid_dataset,\n",
    "    max_num=max(n_subhalos_per_tree),\n",
    "    mode=\"node\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=train_sampler)\n",
    "valid_loader = DataLoader(train_dataset, batch_sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35840a9a-a317-4b94-98d0-9ce58e3d2288",
   "metadata": {},
   "source": [
    "## GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bae698c8-cdb8-4b8e-b75f-d61a64290741",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_IN = tree_data[0].x.shape[1]\n",
    "N_OUT = tree_data[0].y.shape[1]\n",
    "N_HIDDEN = 8\n",
    "N_LAYERS = 8\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5b4fc7b5-8ab6-4d9d-b67a-dc073e2b0508",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiSAGENet(\n",
    "    n_in=N_IN,\n",
    "    n_hidden=N_HIDDEN,\n",
    "    n_layers=N_LAYERS,\n",
    "    n_out=N_OUT\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8ea5a6cd-c1b0-4c72-b8ac-decfb5f01d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3116"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "188e4ae5-cbe5-4b0b-b22e-567255e5e370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizer(model, lr, wd,):\n",
    "    \"\"\"Only apply weight decay to weights, but not to other\n",
    "    parameters like biases or LayerNorm. Based on minGPT version.\n",
    "    \"\"\"\n",
    "\n",
    "    decay, no_decay = set(), set()\n",
    "    yes_wd_modules = (nn.Linear, )\n",
    "    no_wd_modules = (nn.LayerNorm, )\n",
    "    for mn, m in model.named_modules():\n",
    "        for pn, p in m.named_parameters():\n",
    "            fpn = '%s.%s' % (mn, pn) if mn else pn\n",
    "            if pn.endswith('bias'):\n",
    "                no_decay.add(fpn)\n",
    "            elif pn.endswith('weight') and isinstance(m, yes_wd_modules):\n",
    "                decay.add(fpn)\n",
    "            elif pn.endswith('weight') and isinstance(m, no_wd_modules):\n",
    "                no_decay.add(fpn)\n",
    "    param_dict = {pn: p for pn, p in model.named_parameters()}\n",
    "\n",
    "    optim_groups = [\n",
    "        {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": wd},\n",
    "        {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.},\n",
    "    ]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        optim_groups, \n",
    "        lr=lr, \n",
    "    )\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d1ad09fe-d335-4420-a488-1fe966751470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_merger_gnn(\n",
    "    dataloader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: str,\n",
    "    augment: bool = True\n",
    ") -> float:\n",
    "    \"\"\"Train one epoch for GNN model.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: Data loader for training data (X, y tuples)\n",
    "        model: Model to train\n",
    "        optimizer: Optimizer\n",
    "        device: Device to train on\n",
    "        \n",
    "    Returns:\n",
    "        Average training loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_total = 0\n",
    "    n_graphs = 0\n",
    "    for data in (dataloader):\n",
    "        if augment: # add random noise\n",
    "            data_node_features_scatter = 3e-4 * torch.randn_like(data.x[:, :-1]) * torch.std(data.x[:, :-1], dim=0)\n",
    "            data.x[:, :-1] += data_node_features_scatter\n",
    "            assert not torch.isnan(data.x).any() \n",
    "            \n",
    "            if hasattr(data, \"edge_attr\") and data.edge_attr is not None:\n",
    "                data_edge_features_scatter = 3e-4 * torch.randn_like(data.edge_attr) * torch.std(data.edge_attr, dim=0)            \n",
    "                data.edge_attr += data_edge_features_scatter\n",
    "                assert not torch.isnan(data.edge_attr).any() \n",
    "\n",
    "        data.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        y_pred, logvar_pred = output.chunk(2, dim=1)\n",
    "        \n",
    "        assert not torch.isnan(y_pred).any() and not torch.isnan(logvar_pred).any()\n",
    "        \n",
    "        y_pred = y_pred.view(-1, data.y.shape[1] if len(data.y.shape) > 1 else 2)\n",
    "        logvar_pred = logvar_pred.mean()\n",
    "        \n",
    "        loss = gaussian_nll_loss(y_pred, data.y, logvar_pred)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        loss_total += loss.item()\n",
    "        n_graphs += data.y.shape[0]\n",
    "        \n",
    "    return loss_total / n_graphs\n",
    "\n",
    "\n",
    "def validate_merger_gnn(\n",
    "    dataloader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    device: str\n",
    ") -> Tuple[float, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Validate GNN model.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: Validation data loader\n",
    "        model: Model to validate\n",
    "        device: Device to validate on\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (loss, predictions, targets)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    n_graphs = 0\n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        with torch.no_grad():\n",
    "            data.to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            y_pred, logvar_pred = output.chunk(2, dim=1)\n",
    "            \n",
    "            y_pred = y_pred.view(-1, data.y.shape[1] if len(data.y.shape) > 1 else 2)\n",
    "            logvar_pred = logvar_pred.mean()\n",
    "            \n",
    "            loss = gaussian_nll_loss(y_pred, data.y, logvar_pred)\n",
    "            loss_total += loss.item()\n",
    "            n_graphs += data.y.shape[0]\n",
    "            \n",
    "            y_preds.append(y_pred.detach().cpu().numpy())\n",
    "            y_trues.append(data.y.detach().cpu().numpy())\n",
    "    \n",
    "    y_preds = np.concatenate(y_preds, axis=0)\n",
    "    y_trues = np.concatenate(y_trues, axis=0)\n",
    "\n",
    "    \n",
    "    \n",
    "    return loss_total / n_graphs, y_preds, y_trues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d6408abf-9900-4622-a9b1-6e7c5caaf958",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "optimizer = configure_optimizer(model, LEARNING_RATE, WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f36857-a3dd-4e46-8e53-969e7eece069",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 200\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "valid_rmses = []\n",
    "\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss = train_epoch_merger_gnn(train_loader, model, optimizer, device=\"cuda\")\n",
    "    valid_loss, preds, targs = validate_merger_gnn(valid_loader, model, device=\"cuda\")\n",
    "\n",
    "    valid_rmse = compute_rmse(preds, targs)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_rmses.append(valid_rmse)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"{epoch: >3d} {valid_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5711cd7-22f0-4e3d-9114-dee3641258a8",
   "metadata": {},
   "source": [
    "# Putting it all together (full tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0def59a4-33ed-44e8-b4bd-d4b8891036e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spatial_train_valid_indices(data, k: int, K: int = 3, boxsize: float = 75/0.6774, \n",
    "                                   pad: float = 3, indices_mask: Optional[torch.Tensor]=None):\n",
    "    \"\"\"Create spatial train/validation indices using z-coordinate splits.\n",
    "    \n",
    "    This creates spatially separated train/validation sets by dividing the simulation\n",
    "    box along the z-axis. It correctly handles periodic boundaries and optional\n",
    "    pre-filtering to create a true partition of the data.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_indices, valid_indices) as GLOBAL torch tensors, valid\n",
    "        for indexing the original `data` object.\n",
    "    \"\"\"\n",
    "\n",
    "    z_coords = data.pos[:, 2]\n",
    "\n",
    "    valid_start = (k / K * boxsize)\n",
    "    valid_end = ((k + 1) / K * boxsize)\n",
    "    \n",
    "    if k == K - 1:\n",
    "        spatial_valid_mask = (z_coords >= valid_start) | (z_coords < (valid_end % boxsize))\n",
    "    else:\n",
    "        spatial_valid_mask = (z_coords >= valid_start) & (z_coords < valid_end)\n",
    "    \n",
    "    train_start = ((k + 1) / K * boxsize + pad) % boxsize\n",
    "    train_end = (k / K * boxsize - pad) % boxsize\n",
    "    \n",
    "    if train_start > train_end:\n",
    "        spatial_train_mask = (z_coords >= train_start) | (z_coords <= train_end)\n",
    "    else:\n",
    "        spatial_train_mask = (z_coords >= train_start) & (z_coords <= train_end)\n",
    "\n",
    "    if indices_mask is None:\n",
    "        final_mask = torch.ones_like(z_coords, dtype=torch.bool)\n",
    "    else:\n",
    "        final_mask = indices_mask\n",
    "\n",
    "    # Use logical AND to get the final masks for training and validation\n",
    "    final_valid_mask = spatial_valid_mask & final_mask\n",
    "    final_train_mask = spatial_train_mask & final_mask\n",
    "\n",
    "    valid_indices = final_valid_mask.nonzero(as_tuple=True)[0]\n",
    "    train_indices = final_train_mask.nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # Double-check for overlap, which should now be impossible by construction\n",
    "    overlap = set(train_indices.tolist()) & set(valid_indices.tolist())\n",
    "    assert len(overlap) == 0, f\"Found {len(overlap)} overlapping indices\"\n",
    "\n",
    "    print(f\"Fold {k}/{K}: Train={len(train_indices)}, Valid={len(valid_indices)}\")\n",
    "    \n",
    "    return train_indices, valid_indices\n",
    "\n",
    "\n",
    "def gaussian_nll_loss(y_pred: torch.Tensor, y_true: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute Gaussian negative log-likelihood loss *with masking out infinite values*.\n",
    "    \n",
    "    Args:\n",
    "        y_pred: Model predictions\n",
    "        y_true: Ground truth values  \n",
    "        logvar: Log variance predictions\n",
    "        \n",
    "    Returns:\n",
    "        Gaussian NLL loss\n",
    "    \"\"\"\n",
    "    finite_mask = (y_true > 0.) & (y_true.isfinite())\n",
    "    \n",
    "    if not finite_mask.any():\n",
    "        return torch.tensor(0.0, device=y_pred.device, requires_grad=True)\n",
    "    \n",
    "\n",
    "    y_pred_masked = y_pred[finite_mask]\n",
    "    y_true_masked = y_true[finite_mask]\n",
    "    mse_loss = F.mse_loss(y_pred_masked, y_true_masked)\n",
    "    \n",
    "    return 0.5 * (mse_loss / 10**logvar + logvar)\n",
    "\n",
    "\n",
    "def compute_rmse(preds, targs):\n",
    "    \"\"\"lil helper func\"\"\"\n",
    "    finite_mask = (targs > 0.) & (np.isfinite(targs))\n",
    "    y_pred_masked = preds[finite_mask]\n",
    "    y_true_masked = targs[finite_mask]\n",
    "    return np.mean((y_pred_masked - y_true_masked)**2)**0.5\n",
    "\n",
    "\n",
    "def configure_optimizer(model, lr, wd,):\n",
    "    \"\"\"Only apply weight decay to weights, but not to other\n",
    "    parameters like biases or LayerNorm. Based on minGPT version.\n",
    "    \"\"\"\n",
    "\n",
    "    decay, no_decay = set(), set()\n",
    "    yes_wd_modules = (nn.Linear, )\n",
    "    no_wd_modules = (nn.LayerNorm, )\n",
    "    for mn, m in model.named_modules():\n",
    "        for pn, p in m.named_parameters():\n",
    "            fpn = '%s.%s' % (mn, pn) if mn else pn\n",
    "            if pn.endswith('bias'):\n",
    "                no_decay.add(fpn)\n",
    "            elif pn.endswith('weight') and isinstance(m, yes_wd_modules):\n",
    "                decay.add(fpn)\n",
    "            elif pn.endswith('weight') and isinstance(m, no_wd_modules):\n",
    "                no_decay.add(fpn)\n",
    "    param_dict = {pn: p for pn, p in model.named_parameters()}\n",
    "\n",
    "    optim_groups = [\n",
    "        {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": wd},\n",
    "        {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.},\n",
    "    ]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        optim_groups, \n",
    "        lr=lr, \n",
    "    )\n",
    "\n",
    "    return optimizer\n",
    "    \n",
    "def train_epoch_merger_gnn(\n",
    "    dataloader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: str,\n",
    "    augment: bool = True\n",
    ") -> float:\n",
    "    \"\"\"Train one epoch for GNN model.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: Data loader for training data (X, y tuples)\n",
    "        model: Model to train\n",
    "        optimizer: Optimizer\n",
    "        device: Device to train on\n",
    "        \n",
    "    Returns:\n",
    "        Average training loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_total = 0\n",
    "    n_graphs = 0\n",
    "    for data in (dataloader):\n",
    "        if augment: # add random noise\n",
    "            data_node_features_scatter = 3e-4 * torch.randn_like(data.x[:, :-1]) * torch.std(data.x[:, :-1], dim=0)\n",
    "            data.x[:, :-1] += data_node_features_scatter\n",
    "            assert not torch.isnan(data.x).any() \n",
    "            \n",
    "            if hasattr(data, \"edge_attr\") and data.edge_attr is not None:\n",
    "                data_edge_features_scatter = 3e-4 * torch.randn_like(data.edge_attr) * torch.std(data.edge_attr, dim=0)            \n",
    "                data.edge_attr += data_edge_features_scatter\n",
    "                assert not torch.isnan(data.edge_attr).any() \n",
    "\n",
    "        data.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        y_pred, logvar_pred = output.chunk(2, dim=1)\n",
    "        \n",
    "        assert not torch.isnan(y_pred).any() and not torch.isnan(logvar_pred).any()\n",
    "        \n",
    "        y_pred = y_pred.view(-1, data.y.shape[1] if len(data.y.shape) > 1 else 2)\n",
    "        logvar_pred = logvar_pred.mean()\n",
    "        \n",
    "        loss = gaussian_nll_loss(y_pred, data.y, logvar_pred)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        loss_total += loss.item()\n",
    "        n_graphs += data.y.shape[0]\n",
    "        \n",
    "    return loss_total / n_graphs\n",
    "\n",
    "\n",
    "def validate_merger_gnn(\n",
    "    dataloader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    device: str\n",
    ") -> Tuple[float, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Validate GNN model.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: Validation data loader\n",
    "        model: Model to validate\n",
    "        device: Device to validate on\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (loss, predictions, targets)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    n_graphs = 0\n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        with torch.no_grad():\n",
    "            data.to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            y_pred, logvar_pred = output.chunk(2, dim=1)\n",
    "            \n",
    "            y_pred = y_pred.view(-1, data.y.shape[1] if len(data.y.shape) > 1 else 2)\n",
    "            logvar_pred = logvar_pred.mean()\n",
    "            \n",
    "            loss = gaussian_nll_loss(y_pred, data.y, logvar_pred)\n",
    "            loss_total += loss.item()\n",
    "            n_graphs += data.y.shape[0]\n",
    "            \n",
    "            y_preds.append(y_pred.detach().cpu().numpy())\n",
    "            y_trues.append(data.y.detach().cpu().numpy())\n",
    "    \n",
    "    y_preds = np.concatenate(y_preds, axis=0)\n",
    "    y_trues = np.concatenate(y_trues, axis=0)\n",
    "\n",
    "    \n",
    "    \n",
    "    return loss_total / n_graphs, y_preds, y_trues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f8b2788-cb76-409d-9a4d-16cb9a6866a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_FOLDS = 3\n",
    "\n",
    "N_HIDDEN = 32\n",
    "N_LAYERS = 12\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# hyperparams for one-cycle LR schedule (based on Jespersen+ 2022)\n",
    "# PCT_START = 0.15\n",
    "# FINAL_DIV = 1e3\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "DYNAMIC_SAMPLING = False\n",
    "\n",
    "\n",
    "N_EPOCHS = 25\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e095982e-de46-4036-8b8a-9f8bcd3efb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RESULTS_DIR / \"merger_trees.pkl\", \"rb\") as f:\n",
    "    tree_data = pickle.load(f)\n",
    "\n",
    "# need to know for dynamic sampling\n",
    "n_subhalos_per_tree = [tree.x.shape[0] for tree in tree_data]\n",
    "\n",
    "# reshape tree.y! -- also lazy removal of features that are always -inf\n",
    "for tree in tree_data: \n",
    "    tree.x = torch.concatenate([tree.x[:, :2], tree.x[:, 3:4], tree.x[:, 5:]], axis=1)\n",
    "    tree.x[~torch.isfinite(tree.x)] = -3\n",
    "    tree.y = tree.y.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0728c61c-2c36-461e-bc1d-f60dbbd66629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0/3: Train=78822, Valid=36452\n",
      "Fold 1/3: Train=70062, Valid=46397\n",
      "Fold 2/3: Train=75703, Valid=40152\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# use env graph to assign same folds as other experiments\n",
    "# note that this will be a SUBSET of the env graph subhalos!!!\n",
    "with open(RESULTS_DIR /  \"cosmic_graphs_3Mpc.pkl\", \"rb\") as f:\n",
    "    env_data = pickle.load(f)\n",
    "\n",
    "# tree_subhalo_ids = [tree.root_subhalo_id for tree in tree_data]\n",
    "# tree_crossmatches = torch.isin(env_data.subhalo_id, torch.tensor(tree_subhalo_ids))\n",
    "\n",
    "# find which subhalos in env_data have trees\n",
    "all_tree_ids = set(tree.root_subhalo_id for tree in tree_data)\n",
    "tree_crossmatches = torch.tensor([sid.item() in all_tree_ids for sid in env_data.subhalo_id], dtype=torch.bool)\n",
    "\n",
    "\n",
    "train_valid_split = [\n",
    "    get_spatial_train_valid_indices(env_data, k=k, K=K_FOLDS, indices_mask=tree_crossmatches)\n",
    "    for k in range(K_FOLDS)\n",
    "]\n",
    "\n",
    "# metadata\n",
    "subhalo_ids = env_data.subhalo_id[tree_crossmatches]\n",
    "is_central = torch.full_like(subhalo_ids, True, dtype=bool)\n",
    "\n",
    "\n",
    "# create a mapping from subhalo_id to tree for efficient lookup\n",
    "tree_map = {tree.root_subhalo_id: tree for tree in tree_data}\n",
    "\n",
    "N_IN = tree_data[0].x.shape[1]\n",
    "N_OUT = tree_data[0].y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45323a59-b96f-4232-b361-4da25c21d918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 0 Training:   4%|â–ˆ                          | 1/25 [00:55<22:05, 55.24s/it, valid_rmse=0.3090]"
     ]
    }
   ],
   "source": [
    "for k in range(K_FOLDS):\n",
    "    log_file = RESULTS_DIR / f\"logs/tree_gnn_fold_{k}.txt\"\n",
    "    \n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(f\"epoch,train_loss,valid_loss,valid_RMSE\\n\")\n",
    "\n",
    "    # train-valid split & dataloaders\n",
    "    train_indices, valid_indices = train_valid_split[k]\n",
    "    \n",
    "    # map back to subhalo_ids\n",
    "    train_ids_for_fold = env_data.subhalo_id[train_indices].numpy()\n",
    "    valid_ids_for_fold = env_data.subhalo_id[valid_indices].numpy()\n",
    "\n",
    "    is_central_valid_fold = env_data.is_central[valid_indices].flatten().numpy()\n",
    "\n",
    "    train_dataset = [tree_map[sub_id] for sub_id in train_ids_for_fold]\n",
    "    valid_dataset = [tree_map[sub_id] for sub_id in valid_ids_for_fold]\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = MultiSAGENet(\n",
    "        n_in=N_IN,\n",
    "        n_hidden=N_HIDDEN,\n",
    "        n_layers=N_LAYERS,\n",
    "        n_out=N_OUT\n",
    "    )\n",
    "    # model = ModernSAGENet(\n",
    "    #     n_in=N_IN,\n",
    "    #     n_hidden=N_HIDDEN,\n",
    "    #     n_layers=N_LAYERS,\n",
    "    #     n_out=N_OUT\n",
    "    # )\n",
    "    model.to(device);\n",
    "\n",
    "    # optimizer = configure_optimizer(model, LEARNING_RATE, WEIGHT_DECAY)\n",
    "    # scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    #     optimizer,\n",
    "    #     max_lr=LEARNING_RATE,\n",
    "    #     epochs=N_EPOCHS, \n",
    "    #     steps_per_epoch=len(train_loader),\n",
    "    #     pct_start=PCT_START,\n",
    "    #     final_div_factor=FINAL_DIV,\n",
    "    # )\n",
    "\n",
    "    # Muon optimizer\n",
    "    hidden_weights = [p for p in model.parameters() if p.ndim >= 2]\n",
    "    hidden_gains_biases = [p for p in model.parameters() if p.ndim < 2]\n",
    "    param_groups = [\n",
    "        dict(params=hidden_weights, use_muon=True, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY),\n",
    "        dict(params=hidden_gains_biases, use_muon=False, lr=LEARNING_RATE, betas=(0.9, 0.95), weight_decay=0),\n",
    "    ]\n",
    "    \n",
    "    optimizer = SingleDeviceMuonWithAuxAdam(param_groups)\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    valid_rmses = []\n",
    "    \n",
    "    epoch_pbar = tqdm.tqdm(range(N_EPOCHS), desc=f\"Fold {k} Training\", leave=True)\n",
    "    for epoch in epoch_pbar:\n",
    "  \n",
    "        train_loss = train_epoch_merger_gnn(train_loader, model, optimizer, device=device)\n",
    "        valid_loss, preds, targs = validate_merger_gnn(valid_loader, model, device=device)\n",
    "        # scheduler.step()\n",
    "    \n",
    "        valid_rmse = compute_rmse(preds, targs)\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(f\"{epoch:d},{train_loss:.6f},{valid_loss:.6f},{valid_rmse:.6f}\\n\")\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_rmses.append(valid_rmse)\n",
    "    \n",
    "        epoch_pbar.set_postfix({'valid_rmse': f'{valid_rmse:.4f}'})\n",
    "\n",
    "    # save predictions\n",
    "    results_file = RESULTS_DIR / f\"predictions/tree_gnn_fold_{k}.parquet\"\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        \"subhalo_id\": valid_ids_for_fold,\n",
    "        \"log_Mstar_pred\": preds[:, 0],\n",
    "        \"log_Mstar_true\": targs[:, 0],\n",
    "        \"log_Mgas_pred\": preds[:, 1],\n",
    "        \"log_Mgas_true\": targs[:, 1],\n",
    "        \"is_central\": is_central_valid_fold,\n",
    "    }).set_index(\"subhalo_id\")\n",
    "\n",
    "    results_df.to_parquet(results_file)\n",
    "\n",
    "    # save model weights\n",
    "    model_file = RESULTS_DIR / f\"models/tree_gnn_fold_{k}.pth\"\n",
    "    torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882fd91a-997d-4254-b25e-dae9aedf5427",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31152ed4-4f98-4abd-b3f4-37190afc2eec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Training Stumps + Bonsais (jointly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c5119e-b009-4055-8ddd-9f5dfbf59cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spatial_train_valid_indices(data, k: int, K: int = 3, boxsize: float = 75/0.6774, \n",
    "                                   pad: float = 3, indices_mask: Optional[torch.Tensor]=None):\n",
    "    \"\"\"Create spatial train/validation indices using z-coordinate splits.\n",
    "    \n",
    "    This creates spatially separated train/validation sets by dividing the simulation\n",
    "    box along the z-axis. It correctly handles periodic boundaries and optional\n",
    "    pre-filtering to create a true partition of the data.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_indices, valid_indices) as GLOBAL torch tensors, valid\n",
    "        for indexing the original `data` object.\n",
    "    \"\"\"\n",
    "\n",
    "    z_coords = data.pos[:, 2]\n",
    "\n",
    "    valid_start = (k / K * boxsize)\n",
    "    valid_end = ((k + 1) / K * boxsize)\n",
    "    \n",
    "    if k == K - 1:\n",
    "        spatial_valid_mask = (z_coords >= valid_start) | (z_coords < (valid_end % boxsize))\n",
    "    else:\n",
    "        spatial_valid_mask = (z_coords >= valid_start) & (z_coords < valid_end)\n",
    "    \n",
    "    train_start = ((k + 1) / K * boxsize + pad) % boxsize\n",
    "    train_end = (k / K * boxsize - pad) % boxsize\n",
    "    \n",
    "    if train_start > train_end:\n",
    "        spatial_train_mask = (z_coords >= train_start) | (z_coords <= train_end)\n",
    "    else:\n",
    "        spatial_train_mask = (z_coords >= train_start) & (z_coords <= train_end)\n",
    "\n",
    "    if indices_mask is None:\n",
    "        final_mask = torch.ones_like(z_coords, dtype=torch.bool)\n",
    "    else:\n",
    "        final_mask = indices_mask\n",
    "\n",
    "    # Use logical AND to get the final masks for training and validation\n",
    "    final_valid_mask = spatial_valid_mask & final_mask\n",
    "    final_train_mask = spatial_train_mask & final_mask\n",
    "\n",
    "    valid_indices = final_valid_mask.nonzero(as_tuple=True)[0]\n",
    "    train_indices = final_train_mask.nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # Double-check for overlap, which should now be impossible by construction\n",
    "    overlap = set(train_indices.tolist()) & set(valid_indices.tolist())\n",
    "    assert len(overlap) == 0, f\"Found {len(overlap)} overlapping indices\"\n",
    "\n",
    "    print(f\"Fold {k}/{K}: Train={len(train_indices)}, Valid={len(valid_indices)}\")\n",
    "    \n",
    "    return train_indices, valid_indices\n",
    "\n",
    "\n",
    "def gaussian_nll_loss(y_pred: torch.Tensor, y_true: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute Gaussian negative log-likelihood loss *with masking out infinite values*.\n",
    "    \n",
    "    Args:\n",
    "        y_pred: Model predictions\n",
    "        y_true: Ground truth values  \n",
    "        logvar: Log variance predictions\n",
    "        \n",
    "    Returns:\n",
    "        Gaussian NLL loss\n",
    "    \"\"\"\n",
    "    finite_mask = (y_true > 0.) & (y_true.isfinite())\n",
    "    \n",
    "    if not finite_mask.any():\n",
    "        return torch.tensor(0.0, device=y_pred.device, requires_grad=True)\n",
    "    \n",
    "\n",
    "    y_pred_masked = y_pred[finite_mask]\n",
    "    y_true_masked = y_true[finite_mask]\n",
    "    mse_loss = F.mse_loss(y_pred_masked, y_true_masked)\n",
    "    \n",
    "    return 0.5 * (mse_loss / 10**logvar + logvar)\n",
    "\n",
    "\n",
    "def compute_rmse(preds, targs):\n",
    "    \"\"\"lil helper func\"\"\"\n",
    "    finite_mask = (targs > 0.) & (np.isfinite(targs))\n",
    "    y_pred_masked = preds[finite_mask]\n",
    "    y_true_masked = targs[finite_mask]\n",
    "    return np.mean((y_pred_masked - y_true_masked)**2)**0.5\n",
    "\n",
    "\n",
    "def configure_optimizer(model, lr, wd,):\n",
    "    \"\"\"Only apply weight decay to weights, but not to other\n",
    "    parameters like biases or LayerNorm. Based on minGPT version.\n",
    "    \"\"\"\n",
    "\n",
    "    decay, no_decay = set(), set()\n",
    "    yes_wd_modules = (nn.Linear, )\n",
    "    no_wd_modules = (nn.LayerNorm, )\n",
    "    for mn, m in model.named_modules():\n",
    "        for pn, p in m.named_parameters():\n",
    "            fpn = '%s.%s' % (mn, pn) if mn else pn\n",
    "            if pn.endswith('bias'):\n",
    "                no_decay.add(fpn)\n",
    "            elif pn.endswith('weight') and isinstance(m, yes_wd_modules):\n",
    "                decay.add(fpn)\n",
    "            elif pn.endswith('weight') and isinstance(m, no_wd_modules):\n",
    "                no_decay.add(fpn)\n",
    "    param_dict = {pn: p for pn, p in model.named_parameters()}\n",
    "\n",
    "    optim_groups = [\n",
    "        {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": wd},\n",
    "        {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.},\n",
    "    ]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        optim_groups, \n",
    "        lr=lr, \n",
    "    )\n",
    "\n",
    "    return optimizer\n",
    "    \n",
    "def train_epoch_merger_gnn(\n",
    "    dataloader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: str,\n",
    "    augment: bool = True\n",
    ") -> float:\n",
    "    \"\"\"Train one epoch for GNN model.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: Data loader for training data (X, y tuples)\n",
    "        model: Model to train\n",
    "        optimizer: Optimizer\n",
    "        device: Device to train on\n",
    "        \n",
    "    Returns:\n",
    "        Average training loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_total = 0\n",
    "    n_graphs = 0\n",
    "    for data in (dataloader):\n",
    "        if augment: # add random noise\n",
    "            data_node_features_scatter = 3e-4 * torch.randn_like(data.x[:, :-1]) * torch.std(data.x[:, :-1], dim=0)\n",
    "            data.x[:, :-1] += data_node_features_scatter\n",
    "            assert not torch.isnan(data.x).any() \n",
    "            \n",
    "            if hasattr(data, \"edge_attr\") and data.edge_attr is not None:\n",
    "                data_edge_features_scatter = 3e-4 * torch.randn_like(data.edge_attr) * torch.std(data.edge_attr, dim=0)            \n",
    "                data.edge_attr += data_edge_features_scatter\n",
    "                assert not torch.isnan(data.edge_attr).any() \n",
    "\n",
    "        data.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        y_pred, logvar_pred = output.chunk(2, dim=1)\n",
    "        \n",
    "        assert not torch.isnan(y_pred).any() and not torch.isnan(logvar_pred).any()\n",
    "        \n",
    "        y_pred = y_pred.view(-1, data.y.shape[1] if len(data.y.shape) > 1 else 2)\n",
    "        logvar_pred = logvar_pred.mean()\n",
    "        \n",
    "        loss = gaussian_nll_loss(y_pred, data.y, logvar_pred)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        loss_total += loss.item()\n",
    "        n_graphs += data.y.shape[0]\n",
    "        \n",
    "    return loss_total / n_graphs\n",
    "\n",
    "\n",
    "def validate_merger_gnn(\n",
    "    dataloader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    device: str\n",
    ") -> Tuple[float, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Validate GNN model.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: Validation data loader\n",
    "        model: Model to validate\n",
    "        device: Device to validate on\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (loss, predictions, targets)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    n_graphs = 0\n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        with torch.no_grad():\n",
    "            data.to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            y_pred, logvar_pred = output.chunk(2, dim=1)\n",
    "            \n",
    "            y_pred = y_pred.view(-1, data.y.shape[1] if len(data.y.shape) > 1 else 2)\n",
    "            logvar_pred = logvar_pred.mean()\n",
    "            \n",
    "            loss = gaussian_nll_loss(y_pred, data.y, logvar_pred)\n",
    "            loss_total += loss.item()\n",
    "            n_graphs += data.y.shape[0]\n",
    "            \n",
    "            y_preds.append(y_pred.detach().cpu().numpy())\n",
    "            y_trues.append(data.y.detach().cpu().numpy())\n",
    "    \n",
    "    y_preds = np.concatenate(y_preds, axis=0)\n",
    "    y_trues = np.concatenate(y_trues, axis=0)\n",
    "    \n",
    "    return loss_total / n_graphs, y_preds, y_trues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4f3f38-9b6f-485a-a604-ddf038dce286",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RESULTS_DIR / \"cosmic_graphs_3Mpc.pkl\", \"rb\") as f:\n",
    "    env_data = pickle.load(f)\n",
    "\n",
    "with open(RESULTS_DIR / \"merger_tree_bonsais.pkl\", \"rb\") as f:\n",
    "    bonsais = pickle.load(f)\n",
    "\n",
    "with open(RESULTS_DIR / \"merger_tree_stumps.pkl\", \"rb\") as f:\n",
    "    stumps = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5ec92c-758b-4b2f-8c8c-9886d95547d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_data = create_bonsai_stump_pairs(bonsais, stumps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7bd9e4-932f-4a18-aac0-a2d1694c1be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape tree.y! -- also skip columns 2, 4 which are also -inf\n",
    "for tree in tree_data: \n",
    "    tree['bonsai'].x = torch.concatenate([tree[\"bonsai\"].x[:, :2], tree[\"bonsai\"].x[:, 3:4], tree[\"bonsai\"].x[:, 5:]], axis=1)\n",
    "    tree['stump'].x = torch.concatenate([tree[\"stump\"].x[:, :2], tree[\"stump\"].x[:, 3:4], tree[\"stump\"].x[:, 5:]], axis=1)\n",
    "    tree['bonsai'].x[~torch.isfinite(tree['bonsai'].x)] = -3\n",
    "    tree['stump'].x[~torch.isfinite(tree['stump'].x)] = -3\n",
    "    tree.y = tree.y.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cc715b3-d4d7-42b3-b04b-d07b9dae2499",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_FOLDS = 3\n",
    "\n",
    "N_HIDDEN = 32\n",
    "N_LAYERS = 8\n",
    "\n",
    "LEARNING_RATE = 3e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# hyperparams for one-cycle LR schedule (based on Jespersen+ 2022)\n",
    "# PCT_START = 0.15\n",
    "# INIT_DIV = 1e1\n",
    "# FINAL_DIV = 1e3\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "DYNAMIC_SAMPLING = False\n",
    "\n",
    "\n",
    "N_EPOCHS = 25\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dccd86c9-9c62-4a07-a4a9-dfe517fdc264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0/3: Train=78822, Valid=36452\n",
      "Fold 1/3: Train=70062, Valid=46397\n",
      "Fold 2/3: Train=75703, Valid=40152\n"
     ]
    }
   ],
   "source": [
    "# find which subhalos in env_data have trees\n",
    "all_tree_ids = set(tree.root_subhalo_id for tree in tree_data)\n",
    "tree_crossmatches = torch.tensor([sid.item() in all_tree_ids for sid in env_data.subhalo_id], dtype=torch.bool)\n",
    "\n",
    "\n",
    "train_valid_split = [\n",
    "    get_spatial_train_valid_indices(env_data, k=k, K=K_FOLDS, indices_mask=tree_crossmatches)\n",
    "    for k in range(K_FOLDS)\n",
    "]\n",
    "\n",
    "# metadata\n",
    "subhalo_ids = env_data.subhalo_id[tree_crossmatches]\n",
    "is_central = torch.full_like(subhalo_ids, True, dtype=bool)\n",
    "\n",
    "# create a mapping from subhalo_id to tree for efficient lookup\n",
    "tree_map = {tree.root_subhalo_id: tree for tree in tree_data}\n",
    "\n",
    "N_IN = tree_data[0]['bonsai'].x.shape[1]\n",
    "N_OUT = tree_data[0].y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf6d349-c8d9-4e9d-bfc8-d0d04d9c50ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 0 Training:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                   | 7/25 [03:31<09:00, 30.05s/it, valid_rmse=0.3073]"
     ]
    }
   ],
   "source": [
    "for k in range(K_FOLDS):\n",
    "    log_file = RESULTS_DIR / f\"logs/bstree_gnn_fold_{k}.txt\"\n",
    "    \n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(f\"epoch,train_loss,valid_loss,valid_RMSE\\n\")\n",
    "\n",
    "    # train-valid split & dataloaders\n",
    "    train_indices, valid_indices = train_valid_split[k]\n",
    "    \n",
    "    # map back to subhalo_ids\n",
    "    train_ids_for_fold = env_data.subhalo_id[train_indices].numpy()\n",
    "    valid_ids_for_fold = env_data.subhalo_id[valid_indices].numpy()\n",
    "\n",
    "    is_central_valid_fold = env_data.is_central[valid_indices].flatten().numpy()\n",
    "\n",
    "    train_dataset = [tree_map[sub_id] for sub_id in train_ids_for_fold]\n",
    "    valid_dataset = [tree_map[sub_id] for sub_id in valid_ids_for_fold]\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "  \n",
    "    model = ModernBonsaiStumpSAGENet(\n",
    "        n_in=N_IN,\n",
    "        n_hidden=N_HIDDEN,\n",
    "        n_layers=N_LAYERS,\n",
    "        n_out=N_OUT,\n",
    "        act_fn=nn.SiLU()\n",
    "    )\n",
    "    \n",
    "    # model = BonsaiStumpSAGENet(\n",
    "    #     n_in=N_IN,\n",
    "    #     n_hidden=N_HIDDEN,\n",
    "    #     n_layers=N_LAYERS,\n",
    "    #     n_out=N_OUT,\n",
    "    # )\n",
    "    \n",
    "    model.to(device);\n",
    "\n",
    "    # # 1-cycle\n",
    "    # optimizer = configure_optimizer(model, LEARNING_RATE, WEIGHT_DECAY)\n",
    "    # scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    #     optimizer,\n",
    "    #     max_lr=LEARNING_RATE,\n",
    "    #     epochs=N_EPOCHS, \n",
    "    #     steps_per_epoch=len(train_loader),\n",
    "    #     pct_start=PCT_START,\n",
    "    #     div_factor=INIT_DIV,\n",
    "    #     final_div_factor=FINAL_DIV,\n",
    "    # )\n",
    "\n",
    "    # Muon optimizer\n",
    "    hidden_weights = [p for p in model.parameters() if p.ndim >= 2]\n",
    "    hidden_gains_biases = [p for p in model.parameters() if p.ndim < 2]\n",
    "    param_groups = [\n",
    "        dict(params=hidden_weights, use_muon=True, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY),\n",
    "        dict(params=hidden_gains_biases, use_muon=False, lr=LEARNING_RATE, betas=(0.9, 0.95), weight_decay=0),\n",
    "    ]\n",
    "    \n",
    "    optimizer = SingleDeviceMuonWithAuxAdam(param_groups)\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    valid_rmses = []\n",
    "    \n",
    "    epoch_pbar = tqdm.tqdm(range(N_EPOCHS), desc=f\"Fold {k} Training\", leave=True)\n",
    "    for epoch in epoch_pbar:\n",
    "  \n",
    "        train_loss = train_epoch_merger_gnn(train_loader, model, optimizer, augment=False, device=device) # augment doesn't work with the HeteroData\n",
    "        valid_loss, preds, targs = validate_merger_gnn(valid_loader, model, device=device)\n",
    "        # scheduler.step()\n",
    "    \n",
    "        valid_rmse = compute_rmse(preds, targs)\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(f\"{epoch:d},{train_loss:.6f},{valid_loss:.6f},{valid_rmse:.6f}\\n\")\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_rmses.append(valid_rmse)\n",
    "    \n",
    "        epoch_pbar.set_postfix({'valid_rmse': f'{valid_rmse:.4f}'})\n",
    "\n",
    "    # save predictions\n",
    "    results_file = RESULTS_DIR / f\"predictions/bstree_gnn_fold_{k}.parquet\"\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        \"subhalo_id\": valid_ids_for_fold,\n",
    "        \"log_Mstar_pred\": preds[:, 0],\n",
    "        \"log_Mstar_true\": targs[:, 0],\n",
    "        \"log_Mgas_pred\": preds[:, 1],\n",
    "        \"log_Mgas_true\": targs[:, 1],\n",
    "        \"is_central\": is_central_valid_fold,\n",
    "    }).set_index(\"subhalo_id\")\n",
    "\n",
    "    results_df.to_parquet(results_file)\n",
    "\n",
    "    # save model weights\n",
    "    model_file = RESULTS_DIR / f\"models/bstree_gnn_fold_{k}.pth\"\n",
    "    torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b916457-5c0f-48e5-b204-3ec863a1ea0f",
   "metadata": {},
   "source": [
    "# Residual learning with full Merger Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7dea545-6517-43fb-bef1-68b72a6ff6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.loader import DataLoader, DynamicBatchSampler\n",
    "from torch_geometric.nn import MessagePassing, SAGEConv, global_mean_pool, global_max_pool, global_add_pool\n",
    "from torch_geometric.utils import index_to_mask, to_undirected, remove_self_loops\n",
    "from torch_cluster import radius_graph\n",
    "from torch_scatter import scatter_add\n",
    "\n",
    "import tqdm\n",
    "from typing import Tuple, Optional, List\n",
    "\n",
    "from loader import create_bonsai_stump_pairs\n",
    "from model import EdgeInteractionGNN, EdgeInteractionLayer, MultiSAGENet, ModernSAGENet, SAGEEncoder, BonsaiStumpSAGENet, ModernBonsaiStumpSAGENet\n",
    "\n",
    "from muon import SingleDeviceMuonWithAuxAdam\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "BASE_DIR = Path(\"../\")\n",
    "RESULTS_DIR = Path(BASE_DIR / \"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e222273-52f6-4d0f-b675-8e4138f72060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spatial_train_valid_indices(data, k: int, K: int = 3, boxsize: float = 75/0.6774, \n",
    "                                   pad: float = 3, indices_mask: Optional[torch.Tensor]=None):\n",
    "    \"\"\"Create spatial train/validation indices using z-coordinate splits.\n",
    "    \n",
    "    This creates spatially separated train/validation sets by dividing the simulation\n",
    "    box along the z-axis. It correctly handles periodic boundaries and optional\n",
    "    pre-filtering to create a true partition of the data.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_indices, valid_indices) as GLOBAL torch tensors, valid\n",
    "        for indexing the original `data` object.\n",
    "    \"\"\"\n",
    "\n",
    "    z_coords = data.pos[:, 2]\n",
    "\n",
    "    valid_start = (k / K * boxsize)\n",
    "    valid_end = ((k + 1) / K * boxsize)\n",
    "    \n",
    "    if k == K - 1:\n",
    "        spatial_valid_mask = (z_coords >= valid_start) | (z_coords < (valid_end % boxsize))\n",
    "    else:\n",
    "        spatial_valid_mask = (z_coords >= valid_start) & (z_coords < valid_end)\n",
    "    \n",
    "    train_start = ((k + 1) / K * boxsize + pad) % boxsize\n",
    "    train_end = (k / K * boxsize - pad) % boxsize\n",
    "    \n",
    "    if train_start > train_end:\n",
    "        spatial_train_mask = (z_coords >= train_start) | (z_coords <= train_end)\n",
    "    else:\n",
    "        spatial_train_mask = (z_coords >= train_start) & (z_coords <= train_end)\n",
    "\n",
    "    if indices_mask is None:\n",
    "        final_mask = torch.ones_like(z_coords, dtype=torch.bool)\n",
    "    else:\n",
    "        final_mask = indices_mask\n",
    "\n",
    "    # Use logical AND to get the final masks for training and validation\n",
    "    final_valid_mask = spatial_valid_mask & final_mask\n",
    "    final_train_mask = spatial_train_mask & final_mask\n",
    "\n",
    "    valid_indices = final_valid_mask.nonzero(as_tuple=True)[0]\n",
    "    train_indices = final_train_mask.nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # Double-check for overlap, which should now be impossible by construction\n",
    "    overlap = set(train_indices.tolist()) & set(valid_indices.tolist())\n",
    "    assert len(overlap) == 0, f\"Found {len(overlap)} overlapping indices\"\n",
    "\n",
    "    print(f\"Fold {k}/{K}: Train={len(train_indices)}, Valid={len(valid_indices)}\")\n",
    "    \n",
    "    return train_indices, valid_indices\n",
    "\n",
    "\n",
    "def gaussian_nll_loss(y_pred: torch.Tensor, y_true: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute Gaussian negative log-likelihood loss *with masking out infinite values*.\n",
    "    \n",
    "    Args:\n",
    "        y_pred: Model predictions\n",
    "        y_true: Ground truth values  \n",
    "        logvar: Log variance predictions\n",
    "        \n",
    "    Returns:\n",
    "        Gaussian NLL loss\n",
    "    \"\"\"\n",
    "    finite_mask = (y_true > 0.) & (y_true.isfinite())\n",
    "    \n",
    "    if not finite_mask.any():\n",
    "        return torch.tensor(0.0, device=y_pred.device, requires_grad=True)\n",
    "    \n",
    "\n",
    "    y_pred_masked = y_pred[finite_mask]\n",
    "    y_true_masked = y_true[finite_mask]\n",
    "    mse_loss = F.mse_loss(y_pred_masked, y_true_masked)\n",
    "    \n",
    "    return 0.5 * (mse_loss / 10**logvar + logvar)\n",
    "\n",
    "\n",
    "def compute_rmse(preds, targs):\n",
    "    \"\"\"lil helper func\"\"\"\n",
    "    finite_mask = (targs > 0.) & (np.isfinite(targs))\n",
    "    y_pred_masked = preds[finite_mask]\n",
    "    y_true_masked = targs[finite_mask]\n",
    "    return np.mean((y_pred_masked - y_true_masked)**2)**0.5\n",
    "\n",
    "\n",
    "def configure_optimizer(model, lr, wd,):\n",
    "    \"\"\"Only apply weight decay to weights, but not to other\n",
    "    parameters like biases or LayerNorm. Based on minGPT version.\n",
    "    \"\"\"\n",
    "\n",
    "    decay, no_decay = set(), set()\n",
    "    yes_wd_modules = (nn.Linear, )\n",
    "    no_wd_modules = (nn.LayerNorm, )\n",
    "    for mn, m in model.named_modules():\n",
    "        for pn, p in m.named_parameters():\n",
    "            fpn = '%s.%s' % (mn, pn) if mn else pn\n",
    "            if pn.endswith('bias'):\n",
    "                no_decay.add(fpn)\n",
    "            elif pn.endswith('weight') and isinstance(m, yes_wd_modules):\n",
    "                decay.add(fpn)\n",
    "            elif pn.endswith('weight') and isinstance(m, no_wd_modules):\n",
    "                no_decay.add(fpn)\n",
    "    param_dict = {pn: p for pn, p in model.named_parameters()}\n",
    "\n",
    "    optim_groups = [\n",
    "        {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": wd},\n",
    "        {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.},\n",
    "    ]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        optim_groups, \n",
    "        lr=lr, \n",
    "    )\n",
    "\n",
    "    return optimizer\n",
    "    \n",
    "def train_epoch_merger_gnn(\n",
    "    dataloader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: str,\n",
    "    augment: bool = True\n",
    ") -> float:\n",
    "    \"\"\"Train one epoch for GNN model.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: Data loader for training data (X, y tuples)\n",
    "        model: Model to train\n",
    "        optimizer: Optimizer\n",
    "        device: Device to train on\n",
    "        \n",
    "    Returns:\n",
    "        Average training loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_total = 0\n",
    "    n_graphs = 0\n",
    "    for data in (dataloader):\n",
    "        if augment: # add random noise\n",
    "            data_node_features_scatter = 3e-4 * torch.randn_like(data.x[:, :-1]) * torch.std(data.x[:, :-1], dim=0)\n",
    "            data.x[:, :-1] += data_node_features_scatter\n",
    "            assert not torch.isnan(data.x).any() \n",
    "            \n",
    "            if hasattr(data, \"edge_attr\") and data.edge_attr is not None:\n",
    "                data_edge_features_scatter = 3e-4 * torch.randn_like(data.edge_attr) * torch.std(data.edge_attr, dim=0)            \n",
    "                data.edge_attr += data_edge_features_scatter\n",
    "                assert not torch.isnan(data.edge_attr).any() \n",
    "\n",
    "        data.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        y_pred, logvar_pred = output.chunk(2, dim=1)\n",
    "        \n",
    "        assert not torch.isnan(y_pred).any() and not torch.isnan(logvar_pred).any()\n",
    "        \n",
    "        y_pred = y_pred.view(-1, data.y.shape[1] if len(data.y.shape) > 1 else 2)\n",
    "        logvar_pred = logvar_pred.mean()\n",
    "        \n",
    "        loss = gaussian_nll_loss(y_pred, data.y, logvar_pred)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        loss_total += loss.item()\n",
    "        n_graphs += data.y.shape[0]\n",
    "        \n",
    "    return loss_total / n_graphs\n",
    "\n",
    "\n",
    "def validate_merger_gnn(\n",
    "    dataloader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    device: str\n",
    ") -> Tuple[float, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Validate GNN model.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: Validation data loader\n",
    "        model: Model to validate\n",
    "        device: Device to validate on\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (loss, predictions, targets)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    n_graphs = 0\n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        with torch.no_grad():\n",
    "            data.to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            y_pred, logvar_pred = output.chunk(2, dim=1)\n",
    "            \n",
    "            y_pred = y_pred.view(-1, data.y.shape[1] if len(data.y.shape) > 1 else 2)\n",
    "            logvar_pred = logvar_pred.mean()\n",
    "            \n",
    "            loss = gaussian_nll_loss(y_pred, data.y, logvar_pred)\n",
    "            loss_total += loss.item()\n",
    "            n_graphs += data.y.shape[0]\n",
    "            \n",
    "            y_preds.append(y_pred.detach().cpu().numpy())\n",
    "            y_trues.append(data.y.detach().cpu().numpy())\n",
    "    \n",
    "    y_preds = np.concatenate(y_preds, axis=0)\n",
    "    y_trues = np.concatenate(y_trues, axis=0)\n",
    "\n",
    "    \n",
    "    \n",
    "    return loss_total / n_graphs, y_preds, y_trues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cab658bb-7207-42e4-a30f-26c381a53575",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_FOLDS = 3\n",
    "\n",
    "N_HIDDEN = 32\n",
    "N_LAYERS = 12\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# hyperparams for one-cycle LR schedule (based on Jespersen+ 2022)\n",
    "# PCT_START = 0.15\n",
    "# FINAL_DIV = 1e3\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "DYNAMIC_SAMPLING = False\n",
    "\n",
    "\n",
    "N_EPOCHS = 25\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03e46897-5330-42a5-9de3-7f9675b7114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RESULTS_DIR / \"merger_trees.pkl\", \"rb\") as f:\n",
    "    tree_data = pickle.load(f)\n",
    "\n",
    "# need to know for dynamic sampling\n",
    "n_subhalos_per_tree = [tree.x.shape[0] for tree in tree_data]\n",
    "\n",
    "# reshape tree.y! -- also lazy removal of features that are always -inf\n",
    "for tree in tree_data: \n",
    "    tree.x = torch.concatenate([tree.x[:, :2], tree.x[:, 3:4], tree.x[:, 5:]], axis=1)\n",
    "    tree.x[~torch.isfinite(tree.x)] = -3\n",
    "    tree.y = tree.y.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d6ee218-9f21-40cc-9f05-82928027b3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0/3: Train=78822, Valid=36452\n",
      "Fold 1/3: Train=70062, Valid=46397\n",
      "Fold 2/3: Train=75703, Valid=40152\n"
     ]
    }
   ],
   "source": [
    "# use env graph to assign same folds as other experiments\n",
    "# note that this will be a SUBSET of the env graph subhalos!!!\n",
    "with open(RESULTS_DIR /  \"cosmic_graphs_3Mpc.pkl\", \"rb\") as f:\n",
    "    env_data = pickle.load(f)\n",
    "\n",
    "# find which subhalos in env_data have trees\n",
    "all_tree_ids = set(tree.root_subhalo_id for tree in tree_data)\n",
    "tree_crossmatches = torch.tensor([sid.item() in all_tree_ids for sid in env_data.subhalo_id], dtype=torch.bool)\n",
    "\n",
    "\n",
    "train_valid_split = [\n",
    "    get_spatial_train_valid_indices(env_data, k=k, K=K_FOLDS, indices_mask=tree_crossmatches)\n",
    "    for k in range(K_FOLDS)\n",
    "]\n",
    "\n",
    "# create a mapping from subhalo_id to tree for efficient lookup\n",
    "tree_map = {tree.root_subhalo_id: tree for tree in tree_data}\n",
    "\n",
    "N_IN = tree_data[0].x.shape[1]\n",
    "N_OUT = tree_data[0].y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1403529e-7a51-4036-97b8-f00d7dd76c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to residual learning...\n",
    "env_predictions = pd.concat([pd.read_parquet(RESULTS_DIR / f\"predictions/env_gnn_fold_{k}.parquet\") for k in range(3)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89254278-e2bf-418c-94ac-47e1ad3e340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subhalo_id, tree in tree_map.items(): \n",
    "    tree.y_original = tree.y\n",
    "    tree.y = torch.Tensor(env_predictions.loc[subhalo_id][[\"log_Mstar_pred\", \"log_Mgas_pred\"]].values).reshape(1, -1) - tree.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09cb3887-a2df-4f50-88a8-59c80f815163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 0 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [26:19<00:00, 63.19s/it, valid_rmse=0.1952]\n",
      "Fold 1 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [24:43<00:00, 59.35s/it, valid_rmse=0.2124]\n",
      "Fold 2 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [26:12<00:00, 62.90s/it, valid_rmse=0.1985]\n"
     ]
    }
   ],
   "source": [
    "for k in range(K_FOLDS):\n",
    "    log_file = RESULTS_DIR / f\"logs/tree_residual_gnn_fold_{k}.txt\"\n",
    "    \n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(f\"epoch,train_loss,valid_loss,valid_RMSE\\n\")\n",
    "\n",
    "    # train-valid split & dataloaders\n",
    "    train_indices, valid_indices = train_valid_split[k]\n",
    "    \n",
    "    # map back to subhalo_ids\n",
    "    train_ids_for_fold = env_data.subhalo_id[train_indices].numpy()\n",
    "    valid_ids_for_fold = env_data.subhalo_id[valid_indices].numpy()\n",
    "\n",
    "    is_central_valid_fold = env_data.is_central[valid_indices].flatten().numpy()\n",
    "\n",
    "    train_dataset = [tree_map[sub_id] for sub_id in train_ids_for_fold]\n",
    "    valid_dataset = [tree_map[sub_id] for sub_id in valid_ids_for_fold]\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = MultiSAGENet(\n",
    "        n_in=N_IN,\n",
    "        n_hidden=N_HIDDEN,\n",
    "        n_layers=N_LAYERS,\n",
    "        n_out=N_OUT\n",
    "    )\n",
    "    # model = ModernSAGENet(\n",
    "    #     n_in=N_IN,\n",
    "    #     n_hidden=N_HIDDEN,\n",
    "    #     n_layers=N_LAYERS,\n",
    "    #     n_out=N_OUT\n",
    "    # )\n",
    "    model.to(device);\n",
    "\n",
    "    # optimizer = configure_optimizer(model, LEARNING_RATE, WEIGHT_DECAY)\n",
    "    # scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    #     optimizer,\n",
    "    #     max_lr=LEARNING_RATE,\n",
    "    #     epochs=N_EPOCHS, \n",
    "    #     steps_per_epoch=len(train_loader),\n",
    "    #     pct_start=PCT_START,\n",
    "    #     final_div_factor=FINAL_DIV,\n",
    "    # )\n",
    "\n",
    "    # Muon optimizer\n",
    "    hidden_weights = [p for p in model.parameters() if p.ndim >= 2]\n",
    "    hidden_gains_biases = [p for p in model.parameters() if p.ndim < 2]\n",
    "    param_groups = [\n",
    "        dict(params=hidden_weights, use_muon=True, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY),\n",
    "        dict(params=hidden_gains_biases, use_muon=False, lr=LEARNING_RATE, betas=(0.9, 0.95), weight_decay=0),\n",
    "    ]\n",
    "    \n",
    "    optimizer = SingleDeviceMuonWithAuxAdam(param_groups)\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    valid_rmses = []\n",
    "    \n",
    "    epoch_pbar = tqdm.tqdm(range(N_EPOCHS), desc=f\"Fold {k} Training\", leave=True)\n",
    "    for epoch in epoch_pbar:\n",
    "  \n",
    "        train_loss = train_epoch_merger_gnn(train_loader, model, optimizer, device=device)\n",
    "        valid_loss, preds, targs = validate_merger_gnn(valid_loader, model, device=device)\n",
    "        # scheduler.step()\n",
    "    \n",
    "        valid_rmse = compute_rmse(preds, targs)\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(f\"{epoch:d},{train_loss:.6f},{valid_loss:.6f},{valid_rmse:.6f}\\n\")\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_rmses.append(valid_rmse)\n",
    "    \n",
    "        epoch_pbar.set_postfix({'valid_rmse': f'{valid_rmse:.4f}'})\n",
    "\n",
    "    # save predictions\n",
    "    results_file = RESULTS_DIR / f\"predictions/tree_residual_gnn_fold_{k}.parquet\"\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        \"subhalo_id\": valid_ids_for_fold,\n",
    "        \"log_Mstar_pred\": preds[:, 0],\n",
    "        \"log_Mstar_true\": targs[:, 0],\n",
    "        \"log_Mgas_pred\": preds[:, 1],\n",
    "        \"log_Mgas_true\": targs[:, 1],\n",
    "        \"is_central\": is_central_valid_fold,\n",
    "    }).set_index(\"subhalo_id\")\n",
    "\n",
    "    results_df.to_parquet(results_file)\n",
    "\n",
    "    # save model weights\n",
    "    model_file = RESULTS_DIR / f\"models/tree_residual_gnn_fold_{k}.pth\"\n",
    "    torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e1ade1-9607-4f69-9b90-675fd1e95661",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyg]",
   "language": "python",
   "name": "conda-env-pyg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
