{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a9e3d61-3a9d-40b8-a468-af790b9bd69e",
   "metadata": {},
   "source": [
    "# Training MLP models\n",
    "\n",
    "- Training logs saved in `results/logs/mlp_fold_{k}.txt`\n",
    "- Predictions saved in `results/predictions/mlp_fold_{k}.parquet`\n",
    "\n",
    "**Input features:**\n",
    "- subhalo mass\n",
    "- Vmax\n",
    "- is_central\n",
    "\n",
    "**Predictions**\n",
    "- log_Mstar\n",
    "- log_Mgas\n",
    "- log_var(log_Mstar) -- not saved\n",
    "- log_var(log_Mgas) -- not saved\n",
    "\n",
    "**Important notes**\n",
    "- We make an initial cut on subhalos with no valid Mstar or Mgas targets, so there are fewer subhalos (132426) than in the `results/cosmic_graphs_3Mpc.pkl` dataset (132953).\n",
    "- If you want to run the final training analysis (~1.5min per k-fold, or 5 minutes in all), then skip down to the \"All Together Now\" section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fcbe780-a7b7-4ce8-aa6b-39a77f8a3199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tqdm\n",
    "from typing import Tuple, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c392dcd9-9f76-4320-b501-bf6b6291e858",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6972810e-f09e-47a0-9ed2-0e721b4e04bb",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03be8725-5f5c-494d-8187-98fc41a6c0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../results/cosmic_graphs_3Mpc.pkl\", \"rb\") as f:\n",
    "    env_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe7825a5-70a7-4a6e-b34f-f2acaa175f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132426"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_wise_mask = np.logical_and(\n",
    "    np.isfinite(env_data.x).all(axis=1),   # need all inputs\n",
    "    np.isfinite(env_data.y).any(axis=1)    # allowed to have gas mass = NaN\n",
    ").type(torch.bool)\n",
    "\n",
    "row_wise_mask.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c828065-aafe-4bac-9ccc-b1d9786855c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = env_data.x[row_wise_mask]\n",
    "y = env_data.y[row_wise_mask]\n",
    "is_central = env_data.is_central[row_wise_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd265c2d-3d2e-464d-b61c-49af32b4eedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_spatial_train_valid_indices(data, k: int, K: int = 3, boxsize: float = 75/0.6774, \n",
    "#                                    pad: float = 3, epsilon: float = 1e-10, indices_mask=None):\n",
    "#     \"\"\"Create spatial train/validation indices using z-coordinate splits.\n",
    "    \n",
    "#     This creates spatially separated train/validation sets by dividing the simulation\n",
    "#     box along the z-axis. Each fold uses 1/K of the box for validation and the rest\n",
    "#     for training (with padding to avoid boundary effects).\n",
    "    \n",
    "#     Args:\n",
    "#         data: PyTorch Geometric data object with pos attribute\n",
    "#         k: Fold index (0 to K-1)\n",
    "#         K: Total number of folds\n",
    "#         boxsize: Simulation box size in Mpc\n",
    "#         pad: Padding between train/valid regions in Mpc\n",
    "#         epsilon: Small value to avoid boundary issues\n",
    "#         indices_mask: either None or a boolean mask of length X.shape[0]\n",
    "        \n",
    "#     Returns:\n",
    "#         Tuple of (train_indices, valid_indices) as torch tensors\n",
    "#     \"\"\"\n",
    "\n",
    "#     if indices_mask is None:\n",
    "#         z_coords = data.pos[:, 2]\n",
    "#     else:\n",
    "#         z_coords = data.pos[:, 2][indices_mask]\n",
    "\n",
    "#     all_indices = torch.arange(len(z_coords))\n",
    "    \n",
    "    \n",
    "#     # Calculate validation region boundaries\n",
    "#     valid_start = (k / K * boxsize) % boxsize\n",
    "#     valid_end = ((k + 1) / K * boxsize) % boxsize\n",
    "    \n",
    "#     # Handle wrap-around case\n",
    "#     if valid_start > valid_end:  # Wraps around the boundary\n",
    "#         valid_mask = (z_coords >= valid_start) | (z_coords <= valid_end)\n",
    "#     else:\n",
    "#         valid_mask = (z_coords >= valid_start) & (z_coords <= valid_end)\n",
    "    \n",
    "#     # Create training region with padding\n",
    "#     train_start = ((k + 1) / K * boxsize + pad) % boxsize\n",
    "#     train_end = (k / K * boxsize - pad) % boxsize\n",
    "    \n",
    "#     # Handle wrap-around for training region\n",
    "#     if train_start > train_end:  # Wraps around the boundary\n",
    "#         train_mask = (z_coords >= train_start) | (z_coords <= train_end)\n",
    "#     else:\n",
    "#         train_mask = (z_coords >= train_start) & (z_coords <= train_end)\n",
    "\n",
    "\n",
    "#     # Get indices\n",
    "#     train_indices = train_mask.nonzero(as_tuple=True)[0]\n",
    "#     valid_indices = valid_mask.nonzero(as_tuple=True)[0]\n",
    "    \n",
    "#     # Ensure zero overlap\n",
    "#     overlap = set(train_indices.tolist()) & set(valid_indices.tolist())\n",
    "#     assert len(overlap) == 0, f\"Found {len(overlap)} overlapping indices between train and validation\"\n",
    "\n",
    "#     if indices_mask is not None:\n",
    "#         # Convert the boolean mask to a tensor of indices\n",
    "#         allowed_indices = indices_mask.nonzero(as_tuple=True)[0]\n",
    "        \n",
    "#         # Find the intersection of the spatial slice and the allowed indices\n",
    "#         valid_indices = torch.tensor(list(set(valid_indices.tolist()) & set(allowed_indices.tolist())), dtype=torch.long)\n",
    "#         train_indices = torch.tensor(list(set(train_indices.tolist()) & set(allowed_indices.tolist())), dtype=torch.long)\n",
    "        \n",
    "#     print(f\"Fold {k}/{K}: Train={len(train_indices)}, Valid={len(valid_indices)}\")\n",
    "    \n",
    "#     return train_indices, valid_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d237c93-7165-42c9-8b5c-10a5e2a45dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spatial_train_valid_indices(data, k: int, K: int = 3, boxsize: float = 75/0.6774, \n",
    "                                   pad: float = 3, indices_mask: Optional[torch.Tensor]=None):\n",
    "    \"\"\"Create spatial train/validation indices using z-coordinate splits.\n",
    "    \n",
    "    This creates spatially separated train/validation sets by dividing the simulation\n",
    "    box along the z-axis. It correctly handles periodic boundaries and optional\n",
    "    pre-filtering to create a true partition of the data.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_indices, valid_indices) as GLOBAL torch tensors, valid\n",
    "        for indexing the original `data` object.\n",
    "    \"\"\"\n",
    "\n",
    "    z_coords = data.pos[:, 2]\n",
    "\n",
    "    valid_start = (k / K * boxsize)\n",
    "    valid_end = ((k + 1) / K * boxsize)\n",
    "    \n",
    "    if k == K - 1:\n",
    "        spatial_valid_mask = (z_coords >= valid_start) | (z_coords < (valid_end % boxsize))\n",
    "    else:\n",
    "        spatial_valid_mask = (z_coords >= valid_start) & (z_coords < valid_end)\n",
    "    \n",
    "    train_start = ((k + 1) / K * boxsize + pad) % boxsize\n",
    "    train_end = (k / K * boxsize - pad) % boxsize\n",
    "    \n",
    "    if train_start > train_end:\n",
    "        spatial_train_mask = (z_coords >= train_start) | (z_coords <= train_end)\n",
    "    else:\n",
    "        spatial_train_mask = (z_coords >= train_start) & (z_coords <= train_end)\n",
    "\n",
    "    if indices_mask is None:\n",
    "        final_mask = torch.ones_like(z_coords, dtype=torch.bool)\n",
    "    else:\n",
    "        final_mask = indices_mask\n",
    "\n",
    "    # Use logical AND to get the final masks for training and validation\n",
    "    final_valid_mask = spatial_valid_mask & final_mask\n",
    "    final_train_mask = spatial_train_mask & final_mask\n",
    "\n",
    "    # --- Step 4: Convert boolean masks to global indices ---\n",
    "    valid_indices = final_valid_mask.nonzero(as_tuple=True)[0]\n",
    "    train_indices = final_train_mask.nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # Double-check for overlap, which should now be impossible by construction\n",
    "    overlap = set(train_indices.tolist()) & set(valid_indices.tolist())\n",
    "    assert len(overlap) == 0, f\"Found {len(overlap)} overlapping indices\"\n",
    "\n",
    "    print(f\"Fold {k}/{K}: Train={len(train_indices)}, Valid={len(valid_indices)}\")\n",
    "    \n",
    "    return train_indices, valid_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cefaa84f-02f8-4be0-8f6a-ef00e3df15e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0/3: Train=84952, Valid=39685\n",
      "Fold 1/3: Train=75945, Valid=49913\n",
      "Fold 2/3: Train=81916, Valid=43355\n"
     ]
    }
   ],
   "source": [
    "K_FOLDS = 3\n",
    "\n",
    "train_valid_split = [\n",
    "    get_spatial_train_valid_indices(env_data, k=k, K=K_FOLDS, indices_mask=None)\n",
    "    for k in range(K_FOLDS)\n",
    "]\n",
    "\n",
    "assert sum(len(v) for t, v in train_valid_split) == env_data.x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31db8f84-cd6d-4863-ba3e-2314936bc4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0/3: Train=84601, Valid=39562\n",
      "Fold 1/3: Train=75669, Valid=49687\n",
      "Fold 2/3: Train=81609, Valid=43177\n"
     ]
    }
   ],
   "source": [
    "K_FOLDS = 3\n",
    "\n",
    "train_valid_split = [\n",
    "    get_spatial_train_valid_indices(env_data, k=k, K=K_FOLDS, indices_mask=row_wise_mask)\n",
    "    for k in range(K_FOLDS)\n",
    "]\n",
    "\n",
    "assert sum(len(v) for t, v in train_valid_split) == sum(row_wise_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae99596b-93fd-4259-bb52-fb0df90ddbaf",
   "metadata": {},
   "source": [
    "## MLP and training hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98135d1c-9847-43db-a87d-db6f68965e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_mlp(\n",
    "    dataloader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: str\n",
    ") -> float:\n",
    "    \"\"\"Train one epoch for MLP model.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: Data loader for training data (X, y tuples)\n",
    "        model: Model to train\n",
    "        optimizer: Optimizer\n",
    "        device: Device to train on\n",
    "        \n",
    "    Returns:\n",
    "        Average training loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_total = 0\n",
    "    \n",
    "    for X, y in dataloader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "\n",
    "        y_pred, logvar_pred = output.chunk(2, dim=1)\n",
    "        \n",
    "        assert not torch.isnan(y_pred).any() and not torch.isnan(logvar_pred).any()\n",
    "        \n",
    "        y_pred = y_pred.view(-1, y.shape[1] if len(y.shape) > 1 else 1)\n",
    "        logvar_pred = logvar_pred.mean()\n",
    "        \n",
    "        loss = gaussian_nll_loss(y_pred, y, logvar_pred)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        loss_total += loss.item()\n",
    "        \n",
    "    return loss_total / len(dataloader)\n",
    "\n",
    "\n",
    "def validate_mlp(\n",
    "    dataloader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    device: str\n",
    ") -> Tuple[float, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Validate MLP model.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: Validation data loader\n",
    "        model: Model to validate\n",
    "        device: Device to validate on\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (loss, predictions, targets)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "    \n",
    "    for X, y in dataloader:\n",
    "        with torch.no_grad():\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            output = model(X)\n",
    "            y_pred, logvar_pred = output.chunk(2, dim=1)\n",
    "            \n",
    "            y_pred = y_pred.view(-1, y.shape[1] if len(y.shape) > 1 else 1)\n",
    "            logvar_pred = logvar_pred.mean()\n",
    "            \n",
    "            loss = gaussian_nll_loss(y_pred, y, logvar_pred)\n",
    "            loss_total += loss.item()\n",
    "            \n",
    "            y_preds.append(y_pred.detach().cpu().numpy())\n",
    "            y_trues.append(y.detach().cpu().numpy())\n",
    "    \n",
    "    y_preds = np.concatenate(y_preds, axis=0)\n",
    "    y_trues = np.concatenate(y_trues, axis=0)\n",
    "    \n",
    "    return loss_total / len(dataloader), y_preds, y_trues\n",
    "\n",
    "def gaussian_nll_loss(y_pred: torch.Tensor, y_true: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute Gaussian negative log-likelihood loss *with masking out infinite values*.\n",
    "    \n",
    "    Args:\n",
    "        y_pred: Model predictions\n",
    "        y_true: Ground truth values  \n",
    "        logvar: Log variance predictions\n",
    "        \n",
    "    Returns:\n",
    "        Gaussian NLL loss\n",
    "    \"\"\"\n",
    "    finite_mask = (y_true > 0.) & (y_true.isfinite())\n",
    "    \n",
    "    if not finite_mask.any():\n",
    "        return torch.tensor(0.0, device=y_pred.device, requires_grad=True)\n",
    "    \n",
    "\n",
    "    y_pred_masked = y_pred[finite_mask]\n",
    "    y_true_masked = y_true[finite_mask]\n",
    "    mse_loss = F.mse_loss(y_pred_masked, y_true_masked)\n",
    "    \n",
    "    return 0.5 * (mse_loss / 10**logvar + logvar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1808083-e57c-4b2a-b557-7a9c969ba87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_INPUTS = X.shape[1]\n",
    "N_HIDDEN = 128\n",
    "N_OUTPUTS = y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8367cc45-cabb-47f8-9b2d-0ee450bb8bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=3, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=128, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(N_INPUTS, N_HIDDEN),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(N_HIDDEN, N_HIDDEN),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(N_HIDDEN, N_HIDDEN),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(N_HIDDEN, 2*N_OUTPUTS)\n",
    ")\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "083d113e-f678-494d-8a67-ff9ad8b77871",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "N_EPOCHS = 200\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a44cf48-9ac4-4ffe-9f8b-72314d4f8e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e1d2e783-d7ea-4cd7-ba79-5064bbdd3986",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubhaloDataset(Dataset):\n",
    "    def __init__(self, X, y, is_central):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.is_central = is_central\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f01ac011-4abf-47a1-972b-31fd99b20c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "train_indices, valid_indices = train_valid_split[k]\n",
    "\n",
    "train_dataset = SubhaloDataset(X[train_indices], y[train_indices], is_central[train_indices])\n",
    "valid_dataset = SubhaloDataset(X[valid_indices], y[valid_indices], is_central[valid_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "24fe637e-63cf-4b8b-93f7-990a691cca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b48f028b-3727-4cd6-823b-1ca3bf2f7f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(preds, targs):\n",
    "    finite_mask = (targs > 0.) & (np.isfinite(targs))\n",
    "    y_pred_masked = preds[finite_mask]\n",
    "    y_true_masked = targs[finite_mask]\n",
    "    return np.mean((y_pred_masked - y_true_masked)**2)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "918ae87c-91b9-4cf8-b722-2a67034826e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.3778254047206109\n",
      "20 0.3757114615242275\n",
      "30 0.35868102945864944\n",
      "40 0.34507368727894966\n",
      "50 0.33290764205399503\n",
      "60 0.33120742020357347\n",
      "70 0.32710633647519666\n",
      "80 0.3415095293096444\n",
      "90 0.3394796459869219\n",
      "100 0.3262023797304213\n",
      "110 0.32665069064580127\n",
      "120 0.33372600220902604\n",
      "130 0.34176314051228246\n",
      "140 0.32223641234266526\n",
      "150 0.3203132908508958\n",
      "160 0.32449678196655946\n",
      "170 0.3174565625701904\n",
      "180 0.32140523323413805\n",
      "190 0.32300519755111495\n",
      "200 0.32164500037333615\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "valid_losses = []\n",
    "valid_rmses = []\n",
    "\n",
    "for epoch in range(1, N_EPOCHS+1):\n",
    "    train_loss = train_epoch_mlp(train_loader, model, optimizer, device=\"cuda\")\n",
    "    valid_loss, preds, targs = validate_mlp(valid_loader, model, device=\"cuda\")\n",
    "\n",
    "    valid_rmse = compute_rmse(preds, targs)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_rmses.append(valid_rmse)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(epoch, valid_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "daa70dc8-8d9e-49aa-8067-f50c1c49eab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4,4), dpi=150)\n",
    "plt.plot(range(N_EPOCHS), valid_rmses)\n",
    "plt.ylim(0.3, 0.4)\n",
    "plt.grid(alpha=0.15)\n",
    "plt.xlabel(\"Epochs\", fontsize=12)\n",
    "plt.ylabel(\"Validation RMSE [dex]\", fontsize=12);\n",
    "\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd48a2d-e9cb-47cb-b998-d2f7c6b68bf9",
   "metadata": {},
   "source": [
    "## Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "94404c9e-1a4c-46b2-87d1-7aed734985c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4,4), dpi=150)\n",
    "select_centrals = (valid_dataset.is_central).flatten().numpy().astype(bool)\n",
    "\n",
    "plt.scatter(targs[:, 0][select_centrals], preds[:, 0][select_centrals], c=\"C3\", s=3, edgecolor=\"none\", alpha=0.5, )\n",
    "plt.scatter(targs[:, 0][~select_centrals], preds[:, 0][~select_centrals], c=\"C0\", s=3, edgecolor=\"none\", alpha=0.5, )\n",
    "plt.plot([8, 12], [8, 12], ls=\"-\", c=\"k\", lw=1) \n",
    "plt.xlim(8, 12)\n",
    "plt.ylim(8, 12)\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.grid(alpha=0.15)\n",
    "plt.xlabel(r\"True log($M_{\\bigstar}/M_{\\odot}$)\", fontsize=12)\n",
    "plt.ylabel(r\"Predicted log($M_{\\bigstar}/M_{\\odot}$)\", fontsize=12);\n",
    "\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bd67d60e-f775-485f-9d96-cefce7d4eaf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4,4), dpi=150)\n",
    "select_centrals = (valid_dataset.is_central).flatten().numpy().astype(bool)\n",
    "\n",
    "plt.scatter(targs[:, 1][select_centrals], preds[:, 1][select_centrals], c=\"C3\", s=3, edgecolor=\"none\", alpha=0.5, )\n",
    "plt.scatter(targs[:, 1][~select_centrals], preds[:, 1][~select_centrals], c=\"C0\", s=3, edgecolor=\"none\", alpha=0.5, )\n",
    "plt.plot([8, 12], [8, 12], ls=\"-\", c=\"k\", lw=1) \n",
    "plt.xlim(8, 12)\n",
    "plt.ylim(8, 12)\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.grid(alpha=0.15)\n",
    "plt.xlabel(r\"True log($M_{\\rm gas}/M_{\\odot}$)\", fontsize=12)\n",
    "plt.ylabel(r\"Predicted log($M_{\\rm gas}/M_{\\odot}$)\", fontsize=12);\n",
    "\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06228720-d0df-4936-826c-0a0b6e1ed82d",
   "metadata": {},
   "source": [
    "## Print out some RMSE errors..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c38e334b-bd50-434d-a239-f496d425bf15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27665816446725827"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RMSE for log Mstar given a cut on Mstar > 8.5\n",
    "selection = valid_dataset.y[:, 0] > 8.5\n",
    "\n",
    "np.mean(((preds[:, 0] - targs[:, 0])[selection])**2)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6643f2e5-6073-4f82-be44-55faafa4069b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1536048205374387"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RMSE for log Mstar given a cut on Mstar > 8.5 *and CENTRALS*\n",
    "selection = (valid_dataset.y[:, 0] > 8.5) & (valid_dataset.is_central).flatten().numpy()\n",
    "\n",
    "np.mean(((preds[:, 0] - targs[:, 0])[selection])**2)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1235d661-c93f-473e-a037-7711bb841033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33502949494075734"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RMSE for log Mgas given a cut on log Mstar > 8.5 and valid Mgas\n",
    "selection = (targs[:, 0] > 8.5) & torch.isfinite(valid_dataset.y[:, 1]).numpy()\n",
    "\n",
    "np.mean(((preds[:, 1] - targs[:, 1])[selection])**2)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "82e655e9-d607-4412-8e30-8db6acd76305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30505247753148884"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RMSE for log Mgas given a cut on log Mstar > 8.5 and valid Mgas, and for CENTRALS\n",
    "selection = (targs[:, 0] > 8.5) & torch.isfinite(valid_dataset.y[:, 1]).numpy() & (valid_dataset.is_central).flatten().numpy()\n",
    "\n",
    "np.mean(((preds[:, 1] - targs[:, 1])[selection])**2)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3ece4b-cadb-452c-a78d-34ebe79cbcec",
   "metadata": {},
   "source": [
    "# All together now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9b82657-de02-421f-a64d-6edb4e4db4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubhaloDataset(Dataset):\n",
    "    \"\"\"Super simple dataset class for loading into MLP\"\"\"\n",
    "    def __init__(self, X, y, subhalo_id, is_central):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.subhalo_id = subhalo_id\n",
    "        self.is_central = is_central\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "def get_spatial_train_valid_indices(data, k: int, K: int = 3, boxsize: float = 75/0.6774, \n",
    "                                   pad: float = 3, indices_mask: Optional[torch.Tensor]=None):\n",
    "    \"\"\"Create spatial train/validation indices using z-coordinate splits.\n",
    "    \n",
    "    This creates spatially separated train/validation sets by dividing the simulation\n",
    "    box along the z-axis. It correctly handles periodic boundaries and optional\n",
    "    pre-filtering to create a true partition of the data.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_indices, valid_indices) as GLOBAL torch tensors, valid\n",
    "        for indexing the original `data` object.\n",
    "    \"\"\"\n",
    "\n",
    "    z_coords = data.pos[:, 2]\n",
    "\n",
    "    valid_start = (k / K * boxsize)\n",
    "    valid_end = ((k + 1) / K * boxsize)\n",
    "    \n",
    "    if k == K - 1:\n",
    "        spatial_valid_mask = (z_coords >= valid_start) | (z_coords < (valid_end % boxsize))\n",
    "    else:\n",
    "        spatial_valid_mask = (z_coords >= valid_start) & (z_coords < valid_end)\n",
    "    \n",
    "    train_start = ((k + 1) / K * boxsize + pad) % boxsize\n",
    "    train_end = (k / K * boxsize - pad) % boxsize\n",
    "    \n",
    "    if train_start > train_end:\n",
    "        spatial_train_mask = (z_coords >= train_start) | (z_coords <= train_end)\n",
    "    else:\n",
    "        spatial_train_mask = (z_coords >= train_start) & (z_coords <= train_end)\n",
    "\n",
    "    if indices_mask is None:\n",
    "        final_mask = torch.ones_like(z_coords, dtype=torch.bool)\n",
    "    else:\n",
    "        final_mask = indices_mask\n",
    "\n",
    "    # Use logical AND to get the final masks for training and validation\n",
    "    final_valid_mask = spatial_valid_mask & final_mask\n",
    "    final_train_mask = spatial_train_mask & final_mask\n",
    "\n",
    "    valid_indices = final_valid_mask.nonzero(as_tuple=True)[0]\n",
    "    train_indices = final_train_mask.nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # Double-check for overlap, which should now be impossible by construction\n",
    "    overlap = set(train_indices.tolist()) & set(valid_indices.tolist())\n",
    "    assert len(overlap) == 0, f\"Found {len(overlap)} overlapping indices\"\n",
    "\n",
    "    print(f\"Fold {k}/{K}: Train={len(train_indices)}, Valid={len(valid_indices)}\")\n",
    "    \n",
    "    return train_indices, valid_indices\n",
    "\n",
    "\n",
    "def train_epoch_mlp(\n",
    "    dataloader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: str\n",
    ") -> float:\n",
    "    \"\"\"Train one epoch for MLP model.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: Data loader for training data (X, y tuples)\n",
    "        model: Model to train\n",
    "        optimizer: Optimizer\n",
    "        device: Device to train on\n",
    "        \n",
    "    Returns:\n",
    "        Average training loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_total = 0\n",
    "    \n",
    "    for X, y in dataloader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "\n",
    "        y_pred, logvar_pred = output.chunk(2, dim=1)\n",
    "        \n",
    "        assert not torch.isnan(y_pred).any() and not torch.isnan(logvar_pred).any()\n",
    "        \n",
    "        y_pred = y_pred.view(-1, y.shape[1] if len(y.shape) > 1 else 1)\n",
    "        logvar_pred = logvar_pred.mean()\n",
    "        \n",
    "        loss = gaussian_nll_loss(y_pred, y, logvar_pred)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        loss_total += loss.item()\n",
    "        \n",
    "    return loss_total / len(dataloader)\n",
    "\n",
    "\n",
    "def validate_mlp(\n",
    "    dataloader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    device: str\n",
    ") -> Tuple[float, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Validate MLP model.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: Validation data loader\n",
    "        model: Model to validate\n",
    "        device: Device to validate on\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (loss, predictions, targets)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "    \n",
    "    for X, y in dataloader:\n",
    "        with torch.no_grad():\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            output = model(X)\n",
    "            y_pred, logvar_pred = output.chunk(2, dim=1)\n",
    "            \n",
    "            y_pred = y_pred.view(-1, y.shape[1] if len(y.shape) > 1 else 1)\n",
    "            logvar_pred = logvar_pred.mean()\n",
    "            \n",
    "            loss = gaussian_nll_loss(y_pred, y, logvar_pred)\n",
    "            loss_total += loss.item()\n",
    "            \n",
    "            y_preds.append(y_pred.detach().cpu().numpy())\n",
    "            y_trues.append(y.detach().cpu().numpy())\n",
    "    \n",
    "    y_preds = np.concatenate(y_preds, axis=0)\n",
    "    y_trues = np.concatenate(y_trues, axis=0)\n",
    "    \n",
    "    return loss_total / len(dataloader), y_preds, y_trues\n",
    "\n",
    "def gaussian_nll_loss(y_pred: torch.Tensor, y_true: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute Gaussian negative log-likelihood loss *with masking out infinite values*.\n",
    "    \n",
    "    Args:\n",
    "        y_pred: Model predictions\n",
    "        y_true: Ground truth values  \n",
    "        logvar: Log variance predictions\n",
    "        \n",
    "    Returns:\n",
    "        Gaussian NLL loss\n",
    "    \"\"\"\n",
    "    finite_mask = (y_true > 0.) & (y_true.isfinite())\n",
    "    \n",
    "    if not finite_mask.any():\n",
    "        return torch.tensor(0.0, device=y_pred.device, requires_grad=True)\n",
    "    \n",
    "\n",
    "    y_pred_masked = y_pred[finite_mask]\n",
    "    y_true_masked = y_true[finite_mask]\n",
    "    mse_loss = F.mse_loss(y_pred_masked, y_true_masked)\n",
    "    \n",
    "    return 0.5 * (mse_loss / 10**logvar + logvar)\n",
    "\n",
    "\n",
    "def compute_rmse(preds, targs):\n",
    "    \"\"\"lil helper func\"\"\"\n",
    "    finite_mask = (targs > 0.) & (np.isfinite(targs))\n",
    "    y_pred_masked = preds[finite_mask]\n",
    "    y_true_masked = targs[finite_mask]\n",
    "    return np.mean((y_pred_masked - y_true_masked)**2)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae5d7088-02f5-4b12-982e-5601d91bc63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0/3: Train=84601, Valid=39562\n",
      "Fold 1/3: Train=75669, Valid=49687\n",
      "Fold 2/3: Train=81609, Valid=43177\n"
     ]
    }
   ],
   "source": [
    "K_FOLDS = 3\n",
    "\n",
    "with open(\"../results/cosmic_graphs_3Mpc.pkl\", \"rb\") as f:\n",
    "    env_data = pickle.load(f)\n",
    "\n",
    "# mask out completely NaN/inf rows\n",
    "isfinite_mask = np.logical_and(\n",
    "    np.isfinite(env_data.x).all(axis=1),   # need all inputs\n",
    "    np.isfinite(env_data.y).any(axis=1)    # allowed to have gas mass = NaN\n",
    ").type(torch.bool)\n",
    "\n",
    "train_valid_split = [\n",
    "    get_spatial_train_valid_indices(env_data, k=k, K=K_FOLDS, indices_mask=isfinite_mask)\n",
    "    for k in range(K_FOLDS)\n",
    "]\n",
    "\n",
    "assert sum(len(v) for t, v in train_valid_split) == sum(isfinite_mask)\n",
    "\n",
    "X = env_data.x\n",
    "y = env_data.y\n",
    "is_central = env_data.is_central\n",
    "subhalo_ids = env_data.subhalo_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f343fbf-0cd3-49de-955f-67c8d7f6eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MLP hyperparams\n",
    "N_INPUTS = X.shape[1]\n",
    "N_HIDDEN = 128\n",
    "N_OUTPUTS = y.shape[1]\n",
    "\n",
    "# optimization hyperparms\n",
    "LEARNING_RATE = 1e-3\n",
    "N_EPOCHS = 200\n",
    "BATCH_SIZE = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e899cd4-b074-417a-a1f4-af84d7ab7556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 0 Training: 100%|████████████████████████| 200/200 [01:28<00:00,  2.25it/s, valid_rmse=0.3328]\n",
      "Fold 1 Training: 100%|████████████████████████| 200/200 [01:23<00:00,  2.41it/s, valid_rmse=0.3511]\n",
      "Fold 2 Training: 100%|████████████████████████| 200/200 [01:20<00:00,  2.47it/s, valid_rmse=0.3347]\n"
     ]
    }
   ],
   "source": [
    "for k in range(K_FOLDS):\n",
    "    log_file = f\"../results/logs/mlp_fold_{k}.txt\"\n",
    "    \n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(f\"epoch,train_loss,valid_loss,valid_RMSE\\n\")\n",
    "    \n",
    "    train_indices, valid_indices = train_valid_split[k]\n",
    "\n",
    "    X_train, y_train, subhalo_ids_train, is_central_train = X[train_indices], y[train_indices], subhalo_ids[train_indices], is_central[train_indices]\n",
    "    X_valid, y_valid, subhalo_ids_valid, is_central_valid = X[valid_indices], y[valid_indices], subhalo_ids[valid_indices], is_central[valid_indices]\n",
    "    \n",
    "    train_dataset = SubhaloDataset(X_train, y_train, subhalo_ids_train, is_central_train)\n",
    "    valid_dataset = SubhaloDataset(X_valid, y_valid, subhalo_ids_valid, is_central_valid)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(N_INPUTS, N_HIDDEN),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(N_HIDDEN, N_HIDDEN),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(N_HIDDEN, N_HIDDEN),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(N_HIDDEN, 2*N_OUTPUTS)\n",
    "    ).cuda()\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    valid_rmses = []\n",
    "\n",
    "    epoch_pbar = tqdm.tqdm(range(N_EPOCHS), desc=f\"Fold {k} Training\", leave=True)\n",
    "    for epoch in epoch_pbar:\n",
    "        train_loss = train_epoch_mlp(train_loader, model, optimizer, device=\"cuda\")\n",
    "        valid_loss, preds, targs = validate_mlp(valid_loader, model, device=\"cuda\")\n",
    "    \n",
    "        valid_rmse = compute_rmse(preds, targs)\n",
    "\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(f\"{epoch:d},{train_loss:.6f},{valid_loss:.6f},{valid_rmse:.6f}\\n\")\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_rmses.append(valid_rmse)\n",
    "\n",
    "        epoch_pbar.set_postfix({'valid_rmse': f'{valid_rmse:.4f}'})\n",
    "\n",
    "    # save predictions\n",
    "    results_file = f\"../results/predictions/mlp_fold_{k}.parquet\"\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        \"subhalo_id\": subhalo_ids[valid_indices].numpy(),\n",
    "        \"log_Mstar_pred\": preds[:, 0],\n",
    "        \"log_Mstar_true\": targs[:, 0],\n",
    "        \"log_Mgas_pred\": preds[:, 1],\n",
    "        \"log_Mgas_true\": targs[:, 1],\n",
    "        \"is_central\": is_central[valid_indices].flatten().numpy()\n",
    "    }).set_index(\"subhalo_id\")\n",
    "\n",
    "    results_df.to_parquet(results_file)\n",
    "\n",
    "    # save model weights\n",
    "    model_file = f\"../results/models/mlp_fold_{k}.pth\"\n",
    "    torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b0828c-665e-421a-989e-5db098ad4f06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyg]",
   "language": "python",
   "name": "conda-env-pyg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
